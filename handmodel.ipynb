{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torch.optim import Adam\n",
    "import os \n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_train_dir = \"./ann_subsample/ann_subsample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_train_dir = \"./ann_subsample/ann_subsample/\"\n",
    "files = os.listdir(annotation_train_dir)\n",
    "\n",
    "poses = {}\n",
    "\n",
    "for pose in files:\n",
    "    with open(os.path.join(annotation_train_dir,pose)) as f:\n",
    "        data = json.load(f)\n",
    "        poses.update({pose.replace('.json',''): data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "\n",
    "}\n",
    "\n",
    "for pose in poses.keys():\n",
    "    data.update({pose:[]})\n",
    "\n",
    "\n",
    "for pose in poses.keys():\n",
    "    for image in poses[pose].keys():\n",
    "        landmarks = poses[pose][image][\"landmarks\"]\n",
    "        if len(landmarks[0]) == 21:\n",
    "            data[pose].append(torch.tensor(landmarks[0]).squeeze().flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pose in data.keys():\n",
    "    data.update({pose:torch.stack(data[pose])})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class LandmarkSet(Dataset):\n",
    "    def __init__(self, data:Dict[str,torch.Tensor]):\n",
    "        x = torch.tensor([])\n",
    "        y = torch.tensor([])\n",
    "\n",
    "        for index,pose in enumerate(data.keys()):\n",
    "            x = torch.concat((x,data[pose]))\n",
    "            _y = torch.zeros((18))\n",
    "            _y[index] = 1\n",
    "            y = torch.concat((y,_y.repeat((len(data[pose]),1))))\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        return self.x[index],self.y[index]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_set = LandmarkSet(data)\n",
    "length = len(landmark_set)\n",
    "train_data,valid_data,test_data= random_split(landmark_set,[int(x * length) for x in [0.8,0.1,0.1]])# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.3839, 0.3899, 0.3723, 0.3799, 0.3677, 0.3629, 0.3666, 0.3477, 0.3621,\n",
      "         0.3390, 0.3739, 0.3477, 0.3759, 0.3259, 0.3773, 0.3119, 0.3784, 0.3011,\n",
      "         0.3833, 0.3479, 0.3846, 0.3248, 0.3856, 0.3091, 0.3861, 0.2960, 0.3924,\n",
      "         0.3507, 0.3932, 0.3276, 0.3938, 0.3125, 0.3938, 0.3002, 0.4011, 0.3555,\n",
      "         0.4019, 0.3370, 0.4016, 0.3249, 0.4011, 0.3145]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GestureModel = nn.Sequential(\n",
    "    nn.Linear(42,84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84,168),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(168,84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84,42),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(42,21),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(21,18)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train Loss: 2.8918802738189697\n",
      "Epoch 0 valid Loss: 2.903540849685669\n",
      "Epoch 1 train Loss: 2.890639066696167\n",
      "Epoch 1 valid Loss: 2.9025185108184814\n",
      "Epoch 2 train Loss: 2.8897578716278076\n",
      "Epoch 2 valid Loss: 2.9003407955169678\n",
      "Epoch 3 train Loss: 2.8897316455841064\n",
      "Epoch 3 valid Loss: 2.90094256401062\n",
      "Epoch 4 train Loss: 2.8897593021392822\n",
      "Epoch 4 valid Loss: 2.9011247158050537\n",
      "Epoch 5 train Loss: 2.8895468711853027\n",
      "Epoch 5 valid Loss: 2.8988285064697266\n",
      "Epoch 6 train Loss: 2.8901140689849854\n",
      "Epoch 6 valid Loss: 2.8988962173461914\n",
      "Epoch 7 train Loss: 2.8897018432617188\n",
      "Epoch 7 valid Loss: 2.898339033126831\n",
      "Epoch 8 train Loss: 2.8896284103393555\n",
      "Epoch 8 valid Loss: 2.8986265659332275\n",
      "Epoch 9 train Loss: 2.8894448280334473\n",
      "Epoch 9 valid Loss: 2.9004180431365967\n",
      "Epoch 10 train Loss: 2.8895857334136963\n",
      "Epoch 10 valid Loss: 2.898347854614258\n",
      "Epoch 11 train Loss: 2.889143943786621\n",
      "Epoch 11 valid Loss: 2.8980767726898193\n",
      "Epoch 12 train Loss: 2.8893232345581055\n",
      "Epoch 12 valid Loss: 2.897968292236328\n",
      "Epoch 13 train Loss: 2.8895511627197266\n",
      "Epoch 13 valid Loss: 2.9000186920166016\n",
      "Epoch 14 train Loss: 2.889754295349121\n",
      "Epoch 14 valid Loss: 2.900479316711426\n",
      "Epoch 15 train Loss: 2.8894340991973877\n",
      "Epoch 15 valid Loss: 2.9004299640655518\n",
      "Epoch 16 train Loss: 2.889523506164551\n",
      "Epoch 16 valid Loss: 2.9023780822753906\n",
      "Epoch 17 train Loss: 2.890227794647217\n",
      "Epoch 17 valid Loss: 2.900482416152954\n",
      "Epoch 18 train Loss: 2.8898849487304688\n",
      "Epoch 18 valid Loss: 2.8972206115722656\n",
      "Epoch 19 train Loss: 2.8898491859436035\n",
      "Epoch 19 valid Loss: 2.896879196166992\n",
      "Epoch 20 train Loss: 2.8900935649871826\n",
      "Epoch 20 valid Loss: 2.898988723754883\n",
      "Epoch 21 train Loss: 2.8898651599884033\n",
      "Epoch 21 valid Loss: 2.899923324584961\n",
      "Epoch 22 train Loss: 2.8903443813323975\n",
      "Epoch 22 valid Loss: 2.8991193771362305\n",
      "Epoch 23 train Loss: 2.8896470069885254\n",
      "Epoch 23 valid Loss: 2.8987083435058594\n",
      "Epoch 24 train Loss: 2.889714241027832\n",
      "Epoch 24 valid Loss: 2.897670030593872\n",
      "Epoch 25 train Loss: 2.8900606632232666\n",
      "Epoch 25 valid Loss: 2.8988726139068604\n",
      "Epoch 26 train Loss: 2.8898842334747314\n",
      "Epoch 26 valid Loss: 2.8993146419525146\n",
      "Epoch 27 train Loss: 2.890129327774048\n",
      "Epoch 27 valid Loss: 2.8979721069335938\n",
      "Epoch 28 train Loss: 2.889172077178955\n",
      "Epoch 28 valid Loss: 2.898963689804077\n",
      "Epoch 29 train Loss: 2.8899435997009277\n",
      "Epoch 29 valid Loss: 2.89839243888855\n",
      "Epoch 30 train Loss: 2.889564275741577\n",
      "Epoch 30 valid Loss: 2.899888038635254\n",
      "Epoch 31 train Loss: 2.8897523880004883\n",
      "Epoch 31 valid Loss: 2.8975117206573486\n",
      "Epoch 32 train Loss: 2.889364004135132\n",
      "Epoch 32 valid Loss: 2.8993871212005615\n",
      "Epoch 33 train Loss: 2.889695882797241\n",
      "Epoch 33 valid Loss: 2.898883819580078\n",
      "Epoch 34 train Loss: 2.8895492553710938\n",
      "Epoch 34 valid Loss: 2.8993217945098877\n",
      "Epoch 35 train Loss: 2.8896267414093018\n",
      "Epoch 35 valid Loss: 2.90032696723938\n",
      "Epoch 36 train Loss: 2.8896713256835938\n",
      "Epoch 36 valid Loss: 2.898949384689331\n",
      "Epoch 37 train Loss: 2.8899998664855957\n",
      "Epoch 37 valid Loss: 2.899477005004883\n",
      "Epoch 38 train Loss: 2.889416456222534\n",
      "Epoch 38 valid Loss: 2.8969199657440186\n",
      "Epoch 39 train Loss: 2.8896689414978027\n",
      "Epoch 39 valid Loss: 2.8984222412109375\n",
      "Epoch 40 train Loss: 2.8895812034606934\n",
      "Epoch 40 valid Loss: 2.8987762928009033\n",
      "Epoch 41 train Loss: 2.890284538269043\n",
      "Epoch 41 valid Loss: 2.899930238723755\n",
      "Epoch 42 train Loss: 2.890028953552246\n",
      "Epoch 42 valid Loss: 2.8994007110595703\n",
      "Epoch 43 train Loss: 2.889395236968994\n",
      "Epoch 43 valid Loss: 2.899181365966797\n",
      "Epoch 44 train Loss: 2.889699697494507\n",
      "Epoch 44 valid Loss: 2.8978865146636963\n",
      "Epoch 45 train Loss: 2.8897135257720947\n",
      "Epoch 45 valid Loss: 2.8998959064483643\n",
      "Epoch 46 train Loss: 2.889498710632324\n",
      "Epoch 46 valid Loss: 2.900217294692993\n",
      "Epoch 47 train Loss: 2.889418840408325\n",
      "Epoch 47 valid Loss: 2.898341417312622\n",
      "Epoch 48 train Loss: 2.8898098468780518\n",
      "Epoch 48 valid Loss: 2.8990609645843506\n",
      "Epoch 49 train Loss: 2.8897790908813477\n",
      "Epoch 49 valid Loss: 2.8992841243743896\n",
      "Epoch 50 train Loss: 2.8899002075195312\n",
      "Epoch 50 valid Loss: 2.895563840866089\n",
      "Epoch 51 train Loss: 2.8901376724243164\n",
      "Epoch 51 valid Loss: 2.8979651927948\n",
      "Epoch 52 train Loss: 2.889540672302246\n",
      "Epoch 52 valid Loss: 2.8977279663085938\n",
      "Epoch 53 train Loss: 2.889716386795044\n",
      "Epoch 53 valid Loss: 2.8998355865478516\n",
      "Epoch 54 train Loss: 2.889904260635376\n",
      "Epoch 54 valid Loss: 2.8983609676361084\n",
      "Epoch 55 train Loss: 2.8893682956695557\n",
      "Epoch 55 valid Loss: 2.8985979557037354\n",
      "Epoch 56 train Loss: 2.889767646789551\n",
      "Epoch 56 valid Loss: 2.9006187915802\n",
      "Epoch 57 train Loss: 2.889831781387329\n",
      "Epoch 57 valid Loss: 2.8993265628814697\n",
      "Epoch 58 train Loss: 2.8902995586395264\n",
      "Epoch 58 valid Loss: 2.899430274963379\n",
      "Epoch 59 train Loss: 2.8895750045776367\n",
      "Epoch 59 valid Loss: 2.897608757019043\n",
      "Epoch 60 train Loss: 2.8897275924682617\n",
      "Epoch 60 valid Loss: 2.8976829051971436\n",
      "Epoch 61 train Loss: 2.8895859718322754\n",
      "Epoch 61 valid Loss: 2.8976705074310303\n",
      "Epoch 62 train Loss: 2.889594316482544\n",
      "Epoch 62 valid Loss: 2.898902177810669\n",
      "Epoch 63 train Loss: 2.889883518218994\n",
      "Epoch 63 valid Loss: 2.899019956588745\n",
      "Epoch 64 train Loss: 2.8900816440582275\n",
      "Epoch 64 valid Loss: 2.8987839221954346\n",
      "Epoch 65 train Loss: 2.8897790908813477\n",
      "Epoch 65 valid Loss: 2.8981902599334717\n",
      "Epoch 66 train Loss: 2.8894643783569336\n",
      "Epoch 66 valid Loss: 2.897223472595215\n",
      "Epoch 67 train Loss: 2.8899316787719727\n",
      "Epoch 67 valid Loss: 2.897753953933716\n",
      "Epoch 68 train Loss: 2.8903582096099854\n",
      "Epoch 68 valid Loss: 2.8979063034057617\n",
      "Epoch 69 train Loss: 2.8895483016967773\n",
      "Epoch 69 valid Loss: 2.8978652954101562\n",
      "Epoch 70 train Loss: 2.8897910118103027\n",
      "Epoch 70 valid Loss: 2.898388624191284\n",
      "Epoch 71 train Loss: 2.889728307723999\n",
      "Epoch 71 valid Loss: 2.898756265640259\n",
      "Epoch 72 train Loss: 2.8893258571624756\n",
      "Epoch 72 valid Loss: 2.8991997241973877\n",
      "Epoch 73 train Loss: 2.8891334533691406\n",
      "Epoch 73 valid Loss: 2.8977458477020264\n",
      "Epoch 74 train Loss: 2.889867067337036\n",
      "Epoch 74 valid Loss: 2.8976848125457764\n",
      "Epoch 75 train Loss: 2.8899648189544678\n",
      "Epoch 75 valid Loss: 2.90024733543396\n",
      "Epoch 76 train Loss: 2.8900699615478516\n",
      "Epoch 76 valid Loss: 2.899087905883789\n",
      "Epoch 77 train Loss: 2.8894553184509277\n",
      "Epoch 77 valid Loss: 2.8987362384796143\n",
      "Epoch 78 train Loss: 2.8899199962615967\n",
      "Epoch 78 valid Loss: 2.8987748622894287\n",
      "Epoch 79 train Loss: 2.8895366191864014\n",
      "Epoch 79 valid Loss: 2.897843599319458\n",
      "Epoch 80 train Loss: 2.8899271488189697\n",
      "Epoch 80 valid Loss: 2.898102045059204\n",
      "Epoch 81 train Loss: 2.8898651599884033\n",
      "Epoch 81 valid Loss: 2.898332357406616\n",
      "Epoch 82 train Loss: 2.8898661136627197\n",
      "Epoch 82 valid Loss: 2.899444341659546\n",
      "Epoch 83 train Loss: 2.8897898197174072\n",
      "Epoch 83 valid Loss: 2.898055076599121\n",
      "Epoch 84 train Loss: 2.8895187377929688\n",
      "Epoch 84 valid Loss: 2.899165153503418\n",
      "Epoch 85 train Loss: 2.890310525894165\n",
      "Epoch 85 valid Loss: 2.899554491043091\n",
      "Epoch 86 train Loss: 2.889629602432251\n",
      "Epoch 86 valid Loss: 2.8978357315063477\n",
      "Epoch 87 train Loss: 2.8895227909088135\n",
      "Epoch 87 valid Loss: 2.8979737758636475\n",
      "Epoch 88 train Loss: 2.889477014541626\n",
      "Epoch 88 valid Loss: 2.8994102478027344\n",
      "Epoch 89 train Loss: 2.88948655128479\n",
      "Epoch 89 valid Loss: 2.8982133865356445\n",
      "Epoch 90 train Loss: 2.8900816440582275\n",
      "Epoch 90 valid Loss: 2.898521661758423\n",
      "Epoch 91 train Loss: 2.889502763748169\n",
      "Epoch 91 valid Loss: 2.897799491882324\n",
      "Epoch 92 train Loss: 2.889369249343872\n",
      "Epoch 92 valid Loss: 2.898163080215454\n",
      "Epoch 93 train Loss: 2.8899343013763428\n",
      "Epoch 93 valid Loss: 2.8989875316619873\n",
      "Epoch 94 train Loss: 2.889528274536133\n",
      "Epoch 94 valid Loss: 2.8977248668670654\n",
      "Epoch 95 train Loss: 2.8899409770965576\n",
      "Epoch 95 valid Loss: 2.898021697998047\n",
      "Epoch 96 train Loss: 2.8901419639587402\n",
      "Epoch 96 valid Loss: 2.9009130001068115\n",
      "Epoch 97 train Loss: 2.8895514011383057\n",
      "Epoch 97 valid Loss: 2.898300886154175\n",
      "Epoch 98 train Loss: 2.888958215713501\n",
      "Epoch 98 valid Loss: 2.8982417583465576\n",
      "Epoch 99 train Loss: 2.8893918991088867\n",
      "Epoch 99 valid Loss: 2.8969509601593018\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "lr = 1e-2\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = Adam(GestureModel.parameters(),lr=lr)\n",
    "def fit():\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_valid_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            xb,yb = batch\n",
    "            preds = GestureModel(xb)\n",
    "            loss = loss_func(preds,yb)\n",
    "            with torch.no_grad():\n",
    "                epoch_train_loss += loss\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        print(f\"Epoch {epoch} train Loss: {epoch_train_loss/len(train_dataloader)}\")\n",
    "\n",
    "        for batch in valid_dataloader:\n",
    "            with torch.no_grad():\n",
    "                xb,yb = batch\n",
    "                preds = GestureModel(xb)\n",
    "                loss = loss_func(preds,yb)\n",
    "                epoch_valid_loss += loss\n",
    "        \n",
    "        print(f\"Epoch {epoch} valid Loss: {epoch_valid_loss/len(valid_dataloader)}\")\n",
    "\n",
    "\n",
    "\n",
    "fit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    " \n",
    "ce = nn.CrossEntropyLoss()\n",
    " \n",
    "logits = torch.tensor([[-1.90, -0.29, -2.30], [-0.29, -1.90, -2.30]])\n",
    "target = torch.tensor([[0., 1., 0.], [1., 0., 0.]])\n",
    "logits.shape\n",
    "target.shape\n",
    "# print(\"Cross entropy: %.3f\" % ce(logits, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HandGestures-TVPQwZFT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
