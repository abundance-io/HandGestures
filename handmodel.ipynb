{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torch.optim import SGD\n",
    "import os \n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_train_dir = \"./ann_subsample/ann_subsample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_train_dir = \"./ann_subsample/ann_subsample/\"\n",
    "files = os.listdir(annotation_train_dir)\n",
    "\n",
    "poses = {}\n",
    "\n",
    "for pose in files:\n",
    "    with open(os.path.join(annotation_train_dir,pose)) as f:\n",
    "        data = json.load(f)\n",
    "        poses.update({pose.replace('.json',''): data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "\n",
    "}\n",
    "\n",
    "for pose in poses.keys():\n",
    "    data.update({pose:[]})\n",
    "\n",
    "\n",
    "for pose in poses.keys():\n",
    "    for image in poses[pose].keys():\n",
    "        landmarks = poses[pose][image][\"landmarks\"]\n",
    "        if len(landmarks[0]) == 21:\n",
    "            data[pose].append(torch.tensor(landmarks[0]).squeeze().flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pose in data.keys():\n",
    "    data.update({pose:torch.stack(data[pose])})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class LandmarkSet(Dataset):\n",
    "    def __init__(self, data:Dict[str,torch.Tensor]):\n",
    "        x = torch.tensor([])\n",
    "        y = torch.tensor([])\n",
    "\n",
    "        for index,pose in enumerate(data.keys()):\n",
    "            x = torch.concat((x,data[pose]))\n",
    "            _y = torch.zeros((18))\n",
    "            _y[index] = 1\n",
    "            y = torch.concat((y,_y.repeat((len(data[pose]),1))))\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        return self.x[index],self.y[index]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_set = LandmarkSet(data)\n",
    "length = len(landmark_set)\n",
    "train_data,valid_data,test_data= random_split(landmark_set,[int(x * length) for x in [0.8,0.1,0.1]])# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GestureModel = nn.Sequential(\n",
    "    nn.Linear(42,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,21),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(21,18),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train Loss: 2.890371561050415\n",
      "Epoch 0 valid Loss: 2.8903720378875732\n",
      "Epoch 1 train Loss: 2.890371561050415\n",
      "Epoch 1 valid Loss: 2.8903720378875732\n",
      "Epoch 2 train Loss: 2.890371561050415\n",
      "Epoch 2 valid Loss: 2.8903720378875732\n",
      "Epoch 3 train Loss: 2.890371561050415\n",
      "Epoch 3 valid Loss: 2.890371561050415\n",
      "Epoch 4 train Loss: 2.890371561050415\n",
      "Epoch 4 valid Loss: 2.8903720378875732\n",
      "Epoch 5 train Loss: 2.890371561050415\n",
      "Epoch 5 valid Loss: 2.8903720378875732\n",
      "Epoch 6 train Loss: 2.890371561050415\n",
      "Epoch 6 valid Loss: 2.8903720378875732\n",
      "Epoch 7 train Loss: 2.890371561050415\n",
      "Epoch 7 valid Loss: 2.8903720378875732\n",
      "Epoch 8 train Loss: 2.890371561050415\n",
      "Epoch 8 valid Loss: 2.8903720378875732\n",
      "Epoch 9 train Loss: 2.890371561050415\n",
      "Epoch 9 valid Loss: 2.890371561050415\n",
      "Epoch 10 train Loss: 2.890371561050415\n",
      "Epoch 10 valid Loss: 2.8903720378875732\n",
      "Epoch 11 train Loss: 2.890371561050415\n",
      "Epoch 11 valid Loss: 2.8903720378875732\n",
      "Epoch 12 train Loss: 2.890371561050415\n",
      "Epoch 12 valid Loss: 2.890371561050415\n",
      "Epoch 13 train Loss: 2.890371561050415\n",
      "Epoch 13 valid Loss: 2.8903720378875732\n",
      "Epoch 14 train Loss: 2.890371561050415\n",
      "Epoch 14 valid Loss: 2.8903720378875732\n",
      "Epoch 15 train Loss: 2.890371561050415\n",
      "Epoch 15 valid Loss: 2.8903720378875732\n",
      "Epoch 16 train Loss: 2.890371561050415\n",
      "Epoch 16 valid Loss: 2.8903720378875732\n",
      "Epoch 17 train Loss: 2.890371561050415\n",
      "Epoch 17 valid Loss: 2.8903720378875732\n",
      "Epoch 18 train Loss: 2.890371561050415\n",
      "Epoch 18 valid Loss: 2.890371561050415\n",
      "Epoch 19 train Loss: 2.890371561050415\n",
      "Epoch 19 valid Loss: 2.8903720378875732\n",
      "Epoch 20 train Loss: 2.890371561050415\n",
      "Epoch 20 valid Loss: 2.8903720378875732\n",
      "Epoch 21 train Loss: 2.890371561050415\n",
      "Epoch 21 valid Loss: 2.8903720378875732\n",
      "Epoch 22 train Loss: 2.890371561050415\n",
      "Epoch 22 valid Loss: 2.8903720378875732\n",
      "Epoch 23 train Loss: 2.890371561050415\n",
      "Epoch 23 valid Loss: 2.8903720378875732\n",
      "Epoch 24 train Loss: 2.890371561050415\n",
      "Epoch 24 valid Loss: 2.8903720378875732\n",
      "Epoch 25 train Loss: 2.890371561050415\n",
      "Epoch 25 valid Loss: 2.8903720378875732\n",
      "Epoch 26 train Loss: 2.890371561050415\n",
      "Epoch 26 valid Loss: 2.8903720378875732\n",
      "Epoch 27 train Loss: 2.890371561050415\n",
      "Epoch 27 valid Loss: 2.8903720378875732\n",
      "Epoch 28 train Loss: 2.890371561050415\n",
      "Epoch 28 valid Loss: 2.8903720378875732\n",
      "Epoch 29 train Loss: 2.890371561050415\n",
      "Epoch 29 valid Loss: 2.8903720378875732\n",
      "Epoch 30 train Loss: 2.890371561050415\n",
      "Epoch 30 valid Loss: 2.8903720378875732\n",
      "Epoch 31 train Loss: 2.890371561050415\n",
      "Epoch 31 valid Loss: 2.8903720378875732\n",
      "Epoch 32 train Loss: 2.890371561050415\n",
      "Epoch 32 valid Loss: 2.8903720378875732\n",
      "Epoch 33 train Loss: 2.890371561050415\n",
      "Epoch 33 valid Loss: 2.8903720378875732\n",
      "Epoch 34 train Loss: 2.890371561050415\n",
      "Epoch 34 valid Loss: 2.8903720378875732\n",
      "Epoch 35 train Loss: 2.890371561050415\n",
      "Epoch 35 valid Loss: 2.8903720378875732\n",
      "Epoch 36 train Loss: 2.890371561050415\n",
      "Epoch 36 valid Loss: 2.8903720378875732\n",
      "Epoch 37 train Loss: 2.890371561050415\n",
      "Epoch 37 valid Loss: 2.8903720378875732\n",
      "Epoch 38 train Loss: 2.890371561050415\n",
      "Epoch 38 valid Loss: 2.8903720378875732\n",
      "Epoch 39 train Loss: 2.890371561050415\n",
      "Epoch 39 valid Loss: 2.8903720378875732\n",
      "Epoch 40 train Loss: 2.890371561050415\n",
      "Epoch 40 valid Loss: 2.8903720378875732\n",
      "Epoch 41 train Loss: 2.890371561050415\n",
      "Epoch 41 valid Loss: 2.890371561050415\n",
      "Epoch 42 train Loss: 2.890371561050415\n",
      "Epoch 42 valid Loss: 2.8903720378875732\n",
      "Epoch 43 train Loss: 2.890371561050415\n",
      "Epoch 43 valid Loss: 2.890371561050415\n",
      "Epoch 44 train Loss: 2.890371561050415\n",
      "Epoch 44 valid Loss: 2.8903720378875732\n",
      "Epoch 45 train Loss: 2.890371561050415\n",
      "Epoch 45 valid Loss: 2.890371561050415\n",
      "Epoch 46 train Loss: 2.890371561050415\n",
      "Epoch 46 valid Loss: 2.8903720378875732\n",
      "Epoch 47 train Loss: 2.890371561050415\n",
      "Epoch 47 valid Loss: 2.8903720378875732\n",
      "Epoch 48 train Loss: 2.890371561050415\n",
      "Epoch 48 valid Loss: 2.8903720378875732\n",
      "Epoch 49 train Loss: 2.890371561050415\n",
      "Epoch 49 valid Loss: 2.8903720378875732\n",
      "Epoch 50 train Loss: 2.890371561050415\n",
      "Epoch 50 valid Loss: 2.8903720378875732\n",
      "Epoch 51 train Loss: 2.890371561050415\n",
      "Epoch 51 valid Loss: 2.8903720378875732\n",
      "Epoch 52 train Loss: 2.890371561050415\n",
      "Epoch 52 valid Loss: 2.8903720378875732\n",
      "Epoch 53 train Loss: 2.890371561050415\n",
      "Epoch 53 valid Loss: 2.890371561050415\n",
      "Epoch 54 train Loss: 2.890371561050415\n",
      "Epoch 54 valid Loss: 2.8903720378875732\n",
      "Epoch 55 train Loss: 2.890371561050415\n",
      "Epoch 55 valid Loss: 2.8903720378875732\n",
      "Epoch 56 train Loss: 2.890371561050415\n",
      "Epoch 56 valid Loss: 2.8903720378875732\n",
      "Epoch 57 train Loss: 2.890371561050415\n",
      "Epoch 57 valid Loss: 2.890371561050415\n",
      "Epoch 58 train Loss: 2.890371561050415\n",
      "Epoch 58 valid Loss: 2.8903720378875732\n",
      "Epoch 59 train Loss: 2.890371561050415\n",
      "Epoch 59 valid Loss: 2.8903720378875732\n",
      "Epoch 60 train Loss: 2.890371561050415\n",
      "Epoch 60 valid Loss: 2.8903720378875732\n",
      "Epoch 61 train Loss: 2.890371561050415\n",
      "Epoch 61 valid Loss: 2.8903720378875732\n",
      "Epoch 62 train Loss: 2.890371561050415\n",
      "Epoch 62 valid Loss: 2.890371561050415\n",
      "Epoch 63 train Loss: 2.890371561050415\n",
      "Epoch 63 valid Loss: 2.890371561050415\n",
      "Epoch 64 train Loss: 2.890371561050415\n",
      "Epoch 64 valid Loss: 2.8903720378875732\n",
      "Epoch 65 train Loss: 2.890371561050415\n",
      "Epoch 65 valid Loss: 2.8903720378875732\n",
      "Epoch 66 train Loss: 2.890371561050415\n",
      "Epoch 66 valid Loss: 2.8903720378875732\n",
      "Epoch 67 train Loss: 2.890371561050415\n",
      "Epoch 67 valid Loss: 2.8903720378875732\n",
      "Epoch 68 train Loss: 2.890371561050415\n",
      "Epoch 68 valid Loss: 2.8903720378875732\n",
      "Epoch 69 train Loss: 2.890371561050415\n",
      "Epoch 69 valid Loss: 2.8903720378875732\n",
      "Epoch 70 train Loss: 2.890371561050415\n",
      "Epoch 70 valid Loss: 2.8903720378875732\n",
      "Epoch 71 train Loss: 2.890371561050415\n",
      "Epoch 71 valid Loss: 2.8903720378875732\n",
      "Epoch 72 train Loss: 2.890371561050415\n",
      "Epoch 72 valid Loss: 2.8903720378875732\n",
      "Epoch 73 train Loss: 2.890371561050415\n",
      "Epoch 73 valid Loss: 2.890371561050415\n",
      "Epoch 74 train Loss: 2.890371561050415\n",
      "Epoch 74 valid Loss: 2.8903720378875732\n",
      "Epoch 75 train Loss: 2.890371561050415\n",
      "Epoch 75 valid Loss: 2.8903720378875732\n",
      "Epoch 76 train Loss: 2.890371561050415\n",
      "Epoch 76 valid Loss: 2.8903720378875732\n",
      "Epoch 77 train Loss: 2.890371561050415\n",
      "Epoch 77 valid Loss: 2.8903720378875732\n",
      "Epoch 78 train Loss: 2.890371561050415\n",
      "Epoch 78 valid Loss: 2.8903720378875732\n",
      "Epoch 79 train Loss: 2.890371561050415\n",
      "Epoch 79 valid Loss: 2.8903720378875732\n",
      "Epoch 80 train Loss: 2.890371561050415\n",
      "Epoch 80 valid Loss: 2.8903720378875732\n",
      "Epoch 81 train Loss: 2.890371561050415\n",
      "Epoch 81 valid Loss: 2.8903720378875732\n",
      "Epoch 82 train Loss: 2.890371561050415\n",
      "Epoch 82 valid Loss: 2.8903720378875732\n",
      "Epoch 83 train Loss: 2.890371561050415\n",
      "Epoch 83 valid Loss: 2.8903720378875732\n",
      "Epoch 84 train Loss: 2.890371561050415\n",
      "Epoch 84 valid Loss: 2.8903720378875732\n",
      "Epoch 85 train Loss: 2.890371561050415\n",
      "Epoch 85 valid Loss: 2.8903720378875732\n",
      "Epoch 86 train Loss: 2.890371561050415\n",
      "Epoch 86 valid Loss: 2.8903720378875732\n",
      "Epoch 87 train Loss: 2.890371561050415\n",
      "Epoch 87 valid Loss: 2.8903720378875732\n",
      "Epoch 88 train Loss: 2.890371561050415\n",
      "Epoch 88 valid Loss: 2.8903720378875732\n",
      "Epoch 89 train Loss: 2.890371561050415\n",
      "Epoch 89 valid Loss: 2.8903720378875732\n",
      "Epoch 90 train Loss: 2.890371561050415\n",
      "Epoch 90 valid Loss: 2.890371561050415\n",
      "Epoch 91 train Loss: 2.890371561050415\n",
      "Epoch 91 valid Loss: 2.8903720378875732\n",
      "Epoch 92 train Loss: 2.890371799468994\n",
      "Epoch 92 valid Loss: 2.8903720378875732\n",
      "Epoch 93 train Loss: 2.890371561050415\n",
      "Epoch 93 valid Loss: 2.8903720378875732\n",
      "Epoch 94 train Loss: 2.890371561050415\n",
      "Epoch 94 valid Loss: 2.8903720378875732\n",
      "Epoch 95 train Loss: 2.890371561050415\n",
      "Epoch 95 valid Loss: 2.8903720378875732\n",
      "Epoch 96 train Loss: 2.890371561050415\n",
      "Epoch 96 valid Loss: 2.8903720378875732\n",
      "Epoch 97 train Loss: 2.890371561050415\n",
      "Epoch 97 valid Loss: 2.8903720378875732\n",
      "Epoch 98 train Loss: 2.890371561050415\n",
      "Epoch 98 valid Loss: 2.890371561050415\n",
      "Epoch 99 train Loss: 2.890371561050415\n",
      "Epoch 99 valid Loss: 2.890371561050415\n",
      "Epoch 100 train Loss: 2.890371561050415\n",
      "Epoch 100 valid Loss: 2.8903720378875732\n",
      "Epoch 101 train Loss: 2.890371561050415\n",
      "Epoch 101 valid Loss: 2.8903720378875732\n",
      "Epoch 102 train Loss: 2.890371561050415\n",
      "Epoch 102 valid Loss: 2.8903720378875732\n",
      "Epoch 103 train Loss: 2.890371561050415\n",
      "Epoch 103 valid Loss: 2.8903720378875732\n",
      "Epoch 104 train Loss: 2.890371561050415\n",
      "Epoch 104 valid Loss: 2.8903720378875732\n",
      "Epoch 105 train Loss: 2.890371561050415\n",
      "Epoch 105 valid Loss: 2.890371561050415\n",
      "Epoch 106 train Loss: 2.890371561050415\n",
      "Epoch 106 valid Loss: 2.8903720378875732\n",
      "Epoch 107 train Loss: 2.890371561050415\n",
      "Epoch 107 valid Loss: 2.8903720378875732\n",
      "Epoch 108 train Loss: 2.890371561050415\n",
      "Epoch 108 valid Loss: 2.8903720378875732\n",
      "Epoch 109 train Loss: 2.890371561050415\n",
      "Epoch 109 valid Loss: 2.8903720378875732\n",
      "Epoch 110 train Loss: 2.890371561050415\n",
      "Epoch 110 valid Loss: 2.8903720378875732\n",
      "Epoch 111 train Loss: 2.890371561050415\n",
      "Epoch 111 valid Loss: 2.8903720378875732\n",
      "Epoch 112 train Loss: 2.890371561050415\n",
      "Epoch 112 valid Loss: 2.8903720378875732\n",
      "Epoch 113 train Loss: 2.890371561050415\n",
      "Epoch 113 valid Loss: 2.8903720378875732\n",
      "Epoch 114 train Loss: 2.890371561050415\n",
      "Epoch 114 valid Loss: 2.8903720378875732\n",
      "Epoch 115 train Loss: 2.890371561050415\n",
      "Epoch 115 valid Loss: 2.8903720378875732\n",
      "Epoch 116 train Loss: 2.890371561050415\n",
      "Epoch 116 valid Loss: 2.8903720378875732\n",
      "Epoch 117 train Loss: 2.890371561050415\n",
      "Epoch 117 valid Loss: 2.8903720378875732\n",
      "Epoch 118 train Loss: 2.890371561050415\n",
      "Epoch 118 valid Loss: 2.8903720378875732\n",
      "Epoch 119 train Loss: 2.890371561050415\n",
      "Epoch 119 valid Loss: 2.8903720378875732\n",
      "Epoch 120 train Loss: 2.890371561050415\n",
      "Epoch 120 valid Loss: 2.890371561050415\n",
      "Epoch 121 train Loss: 2.890371561050415\n",
      "Epoch 121 valid Loss: 2.8903720378875732\n",
      "Epoch 122 train Loss: 2.890371561050415\n",
      "Epoch 122 valid Loss: 2.890371561050415\n",
      "Epoch 123 train Loss: 2.890371561050415\n",
      "Epoch 123 valid Loss: 2.890371561050415\n",
      "Epoch 124 train Loss: 2.890371561050415\n",
      "Epoch 124 valid Loss: 2.8903720378875732\n",
      "Epoch 125 train Loss: 2.890371561050415\n",
      "Epoch 125 valid Loss: 2.890371561050415\n",
      "Epoch 126 train Loss: 2.890371561050415\n",
      "Epoch 126 valid Loss: 2.8903720378875732\n",
      "Epoch 127 train Loss: 2.890371561050415\n",
      "Epoch 127 valid Loss: 2.8903720378875732\n",
      "Epoch 128 train Loss: 2.890371561050415\n",
      "Epoch 128 valid Loss: 2.8903720378875732\n",
      "Epoch 129 train Loss: 2.890371561050415\n",
      "Epoch 129 valid Loss: 2.8903720378875732\n",
      "Epoch 130 train Loss: 2.890371561050415\n",
      "Epoch 130 valid Loss: 2.8903720378875732\n",
      "Epoch 131 train Loss: 2.890371561050415\n",
      "Epoch 131 valid Loss: 2.8903720378875732\n",
      "Epoch 132 train Loss: 2.890371561050415\n",
      "Epoch 132 valid Loss: 2.890371561050415\n",
      "Epoch 133 train Loss: 2.890371561050415\n",
      "Epoch 133 valid Loss: 2.8903720378875732\n",
      "Epoch 134 train Loss: 2.890371561050415\n",
      "Epoch 134 valid Loss: 2.890371561050415\n",
      "Epoch 135 train Loss: 2.890371561050415\n",
      "Epoch 135 valid Loss: 2.8903720378875732\n",
      "Epoch 136 train Loss: 2.890371561050415\n",
      "Epoch 136 valid Loss: 2.8903720378875732\n",
      "Epoch 137 train Loss: 2.890371561050415\n",
      "Epoch 137 valid Loss: 2.8903720378875732\n",
      "Epoch 138 train Loss: 2.890371561050415\n",
      "Epoch 138 valid Loss: 2.8903720378875732\n",
      "Epoch 139 train Loss: 2.890371561050415\n",
      "Epoch 139 valid Loss: 2.8903720378875732\n",
      "Epoch 140 train Loss: 2.890371561050415\n",
      "Epoch 140 valid Loss: 2.8903720378875732\n",
      "Epoch 141 train Loss: 2.890371561050415\n",
      "Epoch 141 valid Loss: 2.8903720378875732\n",
      "Epoch 142 train Loss: 2.890371799468994\n",
      "Epoch 142 valid Loss: 2.8903720378875732\n",
      "Epoch 143 train Loss: 2.890371561050415\n",
      "Epoch 143 valid Loss: 2.890371561050415\n",
      "Epoch 144 train Loss: 2.890371561050415\n",
      "Epoch 144 valid Loss: 2.8903720378875732\n",
      "Epoch 145 train Loss: 2.890371799468994\n",
      "Epoch 145 valid Loss: 2.8903720378875732\n",
      "Epoch 146 train Loss: 2.890371561050415\n",
      "Epoch 146 valid Loss: 2.8903720378875732\n",
      "Epoch 147 train Loss: 2.890371561050415\n",
      "Epoch 147 valid Loss: 2.8903720378875732\n",
      "Epoch 148 train Loss: 2.890371561050415\n",
      "Epoch 148 valid Loss: 2.8903720378875732\n",
      "Epoch 149 train Loss: 2.890371561050415\n",
      "Epoch 149 valid Loss: 2.8903720378875732\n",
      "Epoch 150 train Loss: 2.890371561050415\n",
      "Epoch 150 valid Loss: 2.8903720378875732\n",
      "Epoch 151 train Loss: 2.890371561050415\n",
      "Epoch 151 valid Loss: 2.8903720378875732\n",
      "Epoch 152 train Loss: 2.890371561050415\n",
      "Epoch 152 valid Loss: 2.8903720378875732\n",
      "Epoch 153 train Loss: 2.890371561050415\n",
      "Epoch 153 valid Loss: 2.8903720378875732\n",
      "Epoch 154 train Loss: 2.890371561050415\n",
      "Epoch 154 valid Loss: 2.8903720378875732\n",
      "Epoch 155 train Loss: 2.890371561050415\n",
      "Epoch 155 valid Loss: 2.8903720378875732\n",
      "Epoch 156 train Loss: 2.890371561050415\n",
      "Epoch 156 valid Loss: 2.8903720378875732\n",
      "Epoch 157 train Loss: 2.890371561050415\n",
      "Epoch 157 valid Loss: 2.8903720378875732\n",
      "Epoch 158 train Loss: 2.890371561050415\n",
      "Epoch 158 valid Loss: 2.8903720378875732\n",
      "Epoch 159 train Loss: 2.890371561050415\n",
      "Epoch 159 valid Loss: 2.890371561050415\n",
      "Epoch 160 train Loss: 2.890371561050415\n",
      "Epoch 160 valid Loss: 2.8903720378875732\n",
      "Epoch 161 train Loss: 2.890371561050415\n",
      "Epoch 161 valid Loss: 2.8903720378875732\n",
      "Epoch 162 train Loss: 2.890371561050415\n",
      "Epoch 162 valid Loss: 2.8903720378875732\n",
      "Epoch 163 train Loss: 2.890371561050415\n",
      "Epoch 163 valid Loss: 2.8903720378875732\n",
      "Epoch 164 train Loss: 2.890371561050415\n",
      "Epoch 164 valid Loss: 2.8903720378875732\n",
      "Epoch 165 train Loss: 2.890371561050415\n",
      "Epoch 165 valid Loss: 2.8903720378875732\n",
      "Epoch 166 train Loss: 2.890371561050415\n",
      "Epoch 166 valid Loss: 2.8903720378875732\n",
      "Epoch 167 train Loss: 2.890371561050415\n",
      "Epoch 167 valid Loss: 2.8903720378875732\n",
      "Epoch 168 train Loss: 2.890371561050415\n",
      "Epoch 168 valid Loss: 2.8903720378875732\n",
      "Epoch 169 train Loss: 2.890371561050415\n",
      "Epoch 169 valid Loss: 2.8903720378875732\n",
      "Epoch 170 train Loss: 2.890371561050415\n",
      "Epoch 170 valid Loss: 2.8903720378875732\n",
      "Epoch 171 train Loss: 2.890371561050415\n",
      "Epoch 171 valid Loss: 2.8903720378875732\n",
      "Epoch 172 train Loss: 2.890371561050415\n",
      "Epoch 172 valid Loss: 2.8903720378875732\n",
      "Epoch 173 train Loss: 2.890371561050415\n",
      "Epoch 173 valid Loss: 2.8903720378875732\n",
      "Epoch 174 train Loss: 2.890371561050415\n",
      "Epoch 174 valid Loss: 2.8903720378875732\n",
      "Epoch 175 train Loss: 2.890371561050415\n",
      "Epoch 175 valid Loss: 2.8903720378875732\n",
      "Epoch 176 train Loss: 2.890371561050415\n",
      "Epoch 176 valid Loss: 2.8903720378875732\n",
      "Epoch 177 train Loss: 2.890371561050415\n",
      "Epoch 177 valid Loss: 2.8903720378875732\n",
      "Epoch 178 train Loss: 2.890371561050415\n",
      "Epoch 178 valid Loss: 2.8903720378875732\n",
      "Epoch 179 train Loss: 2.890371561050415\n",
      "Epoch 179 valid Loss: 2.8903720378875732\n",
      "Epoch 180 train Loss: 2.890371561050415\n",
      "Epoch 180 valid Loss: 2.890371561050415\n",
      "Epoch 181 train Loss: 2.890371561050415\n",
      "Epoch 181 valid Loss: 2.8903720378875732\n",
      "Epoch 182 train Loss: 2.890371561050415\n",
      "Epoch 182 valid Loss: 2.8903720378875732\n",
      "Epoch 183 train Loss: 2.890371561050415\n",
      "Epoch 183 valid Loss: 2.8903720378875732\n",
      "Epoch 184 train Loss: 2.890371561050415\n",
      "Epoch 184 valid Loss: 2.8903720378875732\n",
      "Epoch 185 train Loss: 2.890371561050415\n",
      "Epoch 185 valid Loss: 2.8903720378875732\n",
      "Epoch 186 train Loss: 2.890371561050415\n",
      "Epoch 186 valid Loss: 2.8903720378875732\n",
      "Epoch 187 train Loss: 2.890371561050415\n",
      "Epoch 187 valid Loss: 2.8903720378875732\n",
      "Epoch 188 train Loss: 2.890371561050415\n",
      "Epoch 188 valid Loss: 2.8903720378875732\n",
      "Epoch 189 train Loss: 2.890371561050415\n",
      "Epoch 189 valid Loss: 2.8903720378875732\n",
      "Epoch 190 train Loss: 2.890371561050415\n",
      "Epoch 190 valid Loss: 2.8903720378875732\n",
      "Epoch 191 train Loss: 2.890371561050415\n",
      "Epoch 191 valid Loss: 2.8903720378875732\n",
      "Epoch 192 train Loss: 2.890371561050415\n",
      "Epoch 192 valid Loss: 2.8903720378875732\n",
      "Epoch 193 train Loss: 2.890371561050415\n",
      "Epoch 193 valid Loss: 2.8903720378875732\n",
      "Epoch 194 train Loss: 2.890371561050415\n",
      "Epoch 194 valid Loss: 2.890371561050415\n",
      "Epoch 195 train Loss: 2.890371561050415\n",
      "Epoch 195 valid Loss: 2.8903720378875732\n",
      "Epoch 196 train Loss: 2.890371561050415\n",
      "Epoch 196 valid Loss: 2.8903720378875732\n",
      "Epoch 197 train Loss: 2.890371561050415\n",
      "Epoch 197 valid Loss: 2.8903720378875732\n",
      "Epoch 198 train Loss: 2.890371561050415\n",
      "Epoch 198 valid Loss: 2.8903720378875732\n",
      "Epoch 199 train Loss: 2.890371561050415\n",
      "Epoch 199 valid Loss: 2.8903720378875732\n",
      "Epoch 200 train Loss: 2.890371561050415\n",
      "Epoch 200 valid Loss: 2.8903720378875732\n",
      "Epoch 201 train Loss: 2.890371561050415\n",
      "Epoch 201 valid Loss: 2.8903720378875732\n",
      "Epoch 202 train Loss: 2.890371561050415\n",
      "Epoch 202 valid Loss: 2.8903720378875732\n",
      "Epoch 203 train Loss: 2.890371561050415\n",
      "Epoch 203 valid Loss: 2.890371561050415\n",
      "Epoch 204 train Loss: 2.890371561050415\n",
      "Epoch 204 valid Loss: 2.8903720378875732\n",
      "Epoch 205 train Loss: 2.890371561050415\n",
      "Epoch 205 valid Loss: 2.8903720378875732\n",
      "Epoch 206 train Loss: 2.890371561050415\n",
      "Epoch 206 valid Loss: 2.8903720378875732\n",
      "Epoch 207 train Loss: 2.890371561050415\n",
      "Epoch 207 valid Loss: 2.8903720378875732\n",
      "Epoch 208 train Loss: 2.890371561050415\n",
      "Epoch 208 valid Loss: 2.8903720378875732\n",
      "Epoch 209 train Loss: 2.890371561050415\n",
      "Epoch 209 valid Loss: 2.8903720378875732\n",
      "Epoch 210 train Loss: 2.890371561050415\n",
      "Epoch 210 valid Loss: 2.890371561050415\n",
      "Epoch 211 train Loss: 2.890371561050415\n",
      "Epoch 211 valid Loss: 2.8903720378875732\n",
      "Epoch 212 train Loss: 2.890371561050415\n",
      "Epoch 212 valid Loss: 2.8903720378875732\n",
      "Epoch 213 train Loss: 2.890371561050415\n",
      "Epoch 213 valid Loss: 2.8903720378875732\n",
      "Epoch 214 train Loss: 2.890371561050415\n",
      "Epoch 214 valid Loss: 2.8903720378875732\n",
      "Epoch 215 train Loss: 2.890371561050415\n",
      "Epoch 215 valid Loss: 2.890371561050415\n",
      "Epoch 216 train Loss: 2.890371561050415\n",
      "Epoch 216 valid Loss: 2.8903720378875732\n",
      "Epoch 217 train Loss: 2.890371561050415\n",
      "Epoch 217 valid Loss: 2.8903720378875732\n",
      "Epoch 218 train Loss: 2.890371561050415\n",
      "Epoch 218 valid Loss: 2.8903720378875732\n",
      "Epoch 219 train Loss: 2.890371561050415\n",
      "Epoch 219 valid Loss: 2.890371561050415\n",
      "Epoch 220 train Loss: 2.890371561050415\n",
      "Epoch 220 valid Loss: 2.8903720378875732\n",
      "Epoch 221 train Loss: 2.890371561050415\n",
      "Epoch 221 valid Loss: 2.8903720378875732\n",
      "Epoch 222 train Loss: 2.890371561050415\n",
      "Epoch 222 valid Loss: 2.890371561050415\n",
      "Epoch 223 train Loss: 2.890371561050415\n",
      "Epoch 223 valid Loss: 2.8903720378875732\n",
      "Epoch 224 train Loss: 2.890371561050415\n",
      "Epoch 224 valid Loss: 2.8903720378875732\n",
      "Epoch 225 train Loss: 2.890371561050415\n",
      "Epoch 225 valid Loss: 2.8903720378875732\n",
      "Epoch 226 train Loss: 2.890371561050415\n",
      "Epoch 226 valid Loss: 2.8903720378875732\n",
      "Epoch 227 train Loss: 2.890371561050415\n",
      "Epoch 227 valid Loss: 2.8903720378875732\n",
      "Epoch 228 train Loss: 2.890371561050415\n",
      "Epoch 228 valid Loss: 2.8903720378875732\n",
      "Epoch 229 train Loss: 2.890371561050415\n",
      "Epoch 229 valid Loss: 2.8903720378875732\n",
      "Epoch 230 train Loss: 2.890371561050415\n",
      "Epoch 230 valid Loss: 2.8903720378875732\n",
      "Epoch 231 train Loss: 2.890371561050415\n",
      "Epoch 231 valid Loss: 2.8903720378875732\n",
      "Epoch 232 train Loss: 2.890371561050415\n",
      "Epoch 232 valid Loss: 2.8903720378875732\n",
      "Epoch 233 train Loss: 2.890371561050415\n",
      "Epoch 233 valid Loss: 2.8903720378875732\n",
      "Epoch 234 train Loss: 2.890371561050415\n",
      "Epoch 234 valid Loss: 2.890371561050415\n",
      "Epoch 235 train Loss: 2.890371561050415\n",
      "Epoch 235 valid Loss: 2.8903720378875732\n",
      "Epoch 236 train Loss: 2.890371561050415\n",
      "Epoch 236 valid Loss: 2.8903720378875732\n",
      "Epoch 237 train Loss: 2.890371561050415\n",
      "Epoch 237 valid Loss: 2.890371561050415\n",
      "Epoch 238 train Loss: 2.890371561050415\n",
      "Epoch 238 valid Loss: 2.8903720378875732\n",
      "Epoch 239 train Loss: 2.890371561050415\n",
      "Epoch 239 valid Loss: 2.8903720378875732\n",
      "Epoch 240 train Loss: 2.890371561050415\n",
      "Epoch 240 valid Loss: 2.8903720378875732\n",
      "Epoch 241 train Loss: 2.890371561050415\n",
      "Epoch 241 valid Loss: 2.890371561050415\n",
      "Epoch 242 train Loss: 2.890371561050415\n",
      "Epoch 242 valid Loss: 2.8903720378875732\n",
      "Epoch 243 train Loss: 2.890371561050415\n",
      "Epoch 243 valid Loss: 2.890371561050415\n",
      "Epoch 244 train Loss: 2.890371561050415\n",
      "Epoch 244 valid Loss: 2.8903720378875732\n",
      "Epoch 245 train Loss: 2.890371561050415\n",
      "Epoch 245 valid Loss: 2.890371561050415\n",
      "Epoch 246 train Loss: 2.890371561050415\n",
      "Epoch 246 valid Loss: 2.8903720378875732\n",
      "Epoch 247 train Loss: 2.890371561050415\n",
      "Epoch 247 valid Loss: 2.8903720378875732\n",
      "Epoch 248 train Loss: 2.890371561050415\n",
      "Epoch 248 valid Loss: 2.8903720378875732\n",
      "Epoch 249 train Loss: 2.890371561050415\n",
      "Epoch 249 valid Loss: 2.8903720378875732\n",
      "Epoch 250 train Loss: 2.890371561050415\n",
      "Epoch 250 valid Loss: 2.8903720378875732\n",
      "Epoch 251 train Loss: 2.890371561050415\n",
      "Epoch 251 valid Loss: 2.8903720378875732\n",
      "Epoch 252 train Loss: 2.890371561050415\n",
      "Epoch 252 valid Loss: 2.8903720378875732\n",
      "Epoch 253 train Loss: 2.890371561050415\n",
      "Epoch 253 valid Loss: 2.8903720378875732\n",
      "Epoch 254 train Loss: 2.890371561050415\n",
      "Epoch 254 valid Loss: 2.8903720378875732\n",
      "Epoch 255 train Loss: 2.890371561050415\n",
      "Epoch 255 valid Loss: 2.8903720378875732\n",
      "Epoch 256 train Loss: 2.890371561050415\n",
      "Epoch 256 valid Loss: 2.8903720378875732\n",
      "Epoch 257 train Loss: 2.890371561050415\n",
      "Epoch 257 valid Loss: 2.8903720378875732\n",
      "Epoch 258 train Loss: 2.890371561050415\n",
      "Epoch 258 valid Loss: 2.8903720378875732\n",
      "Epoch 259 train Loss: 2.890371561050415\n",
      "Epoch 259 valid Loss: 2.8903720378875732\n",
      "Epoch 260 train Loss: 2.890371561050415\n",
      "Epoch 260 valid Loss: 2.8903720378875732\n",
      "Epoch 261 train Loss: 2.890371561050415\n",
      "Epoch 261 valid Loss: 2.8903720378875732\n",
      "Epoch 262 train Loss: 2.890371561050415\n",
      "Epoch 262 valid Loss: 2.8903720378875732\n",
      "Epoch 263 train Loss: 2.890371561050415\n",
      "Epoch 263 valid Loss: 2.8903720378875732\n",
      "Epoch 264 train Loss: 2.890371561050415\n",
      "Epoch 264 valid Loss: 2.8903720378875732\n",
      "Epoch 265 train Loss: 2.890371561050415\n",
      "Epoch 265 valid Loss: 2.8903720378875732\n",
      "Epoch 266 train Loss: 2.890371561050415\n",
      "Epoch 266 valid Loss: 2.8903720378875732\n",
      "Epoch 267 train Loss: 2.890371561050415\n",
      "Epoch 267 valid Loss: 2.8903720378875732\n",
      "Epoch 268 train Loss: 2.890371561050415\n",
      "Epoch 268 valid Loss: 2.8903720378875732\n",
      "Epoch 269 train Loss: 2.890371561050415\n",
      "Epoch 269 valid Loss: 2.8903720378875732\n",
      "Epoch 270 train Loss: 2.890371561050415\n",
      "Epoch 270 valid Loss: 2.8903720378875732\n",
      "Epoch 271 train Loss: 2.890371561050415\n",
      "Epoch 271 valid Loss: 2.8903720378875732\n",
      "Epoch 272 train Loss: 2.890371561050415\n",
      "Epoch 272 valid Loss: 2.8903720378875732\n",
      "Epoch 273 train Loss: 2.890371561050415\n",
      "Epoch 273 valid Loss: 2.8903720378875732\n",
      "Epoch 274 train Loss: 2.890371561050415\n",
      "Epoch 274 valid Loss: 2.8903720378875732\n",
      "Epoch 275 train Loss: 2.890371561050415\n",
      "Epoch 275 valid Loss: 2.8903720378875732\n",
      "Epoch 276 train Loss: 2.890371561050415\n",
      "Epoch 276 valid Loss: 2.8903720378875732\n",
      "Epoch 277 train Loss: 2.890371561050415\n",
      "Epoch 277 valid Loss: 2.8903720378875732\n",
      "Epoch 278 train Loss: 2.890371561050415\n",
      "Epoch 278 valid Loss: 2.8903720378875732\n",
      "Epoch 279 train Loss: 2.890371561050415\n",
      "Epoch 279 valid Loss: 2.8903720378875732\n",
      "Epoch 280 train Loss: 2.890371561050415\n",
      "Epoch 280 valid Loss: 2.8903720378875732\n",
      "Epoch 281 train Loss: 2.890371561050415\n",
      "Epoch 281 valid Loss: 2.8903720378875732\n",
      "Epoch 282 train Loss: 2.890371561050415\n",
      "Epoch 282 valid Loss: 2.8903720378875732\n",
      "Epoch 283 train Loss: 2.890371561050415\n",
      "Epoch 283 valid Loss: 2.890371561050415\n",
      "Epoch 284 train Loss: 2.890371561050415\n",
      "Epoch 284 valid Loss: 2.8903720378875732\n",
      "Epoch 285 train Loss: 2.890371561050415\n",
      "Epoch 285 valid Loss: 2.8903720378875732\n",
      "Epoch 286 train Loss: 2.890371561050415\n",
      "Epoch 286 valid Loss: 2.8903720378875732\n",
      "Epoch 287 train Loss: 2.890371561050415\n",
      "Epoch 287 valid Loss: 2.8903720378875732\n",
      "Epoch 288 train Loss: 2.890371561050415\n",
      "Epoch 288 valid Loss: 2.8903720378875732\n",
      "Epoch 289 train Loss: 2.890371561050415\n",
      "Epoch 289 valid Loss: 2.8903720378875732\n",
      "Epoch 290 train Loss: 2.890371561050415\n",
      "Epoch 290 valid Loss: 2.8903720378875732\n",
      "Epoch 291 train Loss: 2.890371561050415\n",
      "Epoch 291 valid Loss: 2.8903720378875732\n",
      "Epoch 292 train Loss: 2.890371561050415\n",
      "Epoch 292 valid Loss: 2.8903720378875732\n",
      "Epoch 293 train Loss: 2.890371561050415\n",
      "Epoch 293 valid Loss: 2.8903720378875732\n",
      "Epoch 294 train Loss: 2.890371561050415\n",
      "Epoch 294 valid Loss: 2.8903720378875732\n",
      "Epoch 295 train Loss: 2.890371561050415\n",
      "Epoch 295 valid Loss: 2.8903720378875732\n",
      "Epoch 296 train Loss: 2.890371561050415\n",
      "Epoch 296 valid Loss: 2.8903720378875732\n",
      "Epoch 297 train Loss: 2.890371561050415\n",
      "Epoch 297 valid Loss: 2.8903720378875732\n",
      "Epoch 298 train Loss: 2.890371561050415\n",
      "Epoch 298 valid Loss: 2.890371561050415\n",
      "Epoch 299 train Loss: 2.890371561050415\n",
      "Epoch 299 valid Loss: 2.8903720378875732\n",
      "Epoch 300 train Loss: 2.890371561050415\n",
      "Epoch 300 valid Loss: 2.8903720378875732\n",
      "Epoch 301 train Loss: 2.890371561050415\n",
      "Epoch 301 valid Loss: 2.8903720378875732\n",
      "Epoch 302 train Loss: 2.890371561050415\n",
      "Epoch 302 valid Loss: 2.8903720378875732\n",
      "Epoch 303 train Loss: 2.890371561050415\n",
      "Epoch 303 valid Loss: 2.8903720378875732\n",
      "Epoch 304 train Loss: 2.890371561050415\n",
      "Epoch 304 valid Loss: 2.8903720378875732\n",
      "Epoch 305 train Loss: 2.890371561050415\n",
      "Epoch 305 valid Loss: 2.8903720378875732\n",
      "Epoch 306 train Loss: 2.890371561050415\n",
      "Epoch 306 valid Loss: 2.8903720378875732\n",
      "Epoch 307 train Loss: 2.890371561050415\n",
      "Epoch 307 valid Loss: 2.8903720378875732\n",
      "Epoch 308 train Loss: 2.890371561050415\n",
      "Epoch 308 valid Loss: 2.8903720378875732\n",
      "Epoch 309 train Loss: 2.890371561050415\n",
      "Epoch 309 valid Loss: 2.8903720378875732\n",
      "Epoch 310 train Loss: 2.890371561050415\n",
      "Epoch 310 valid Loss: 2.8903720378875732\n",
      "Epoch 311 train Loss: 2.890371561050415\n",
      "Epoch 311 valid Loss: 2.8903720378875732\n",
      "Epoch 312 train Loss: 2.890371561050415\n",
      "Epoch 312 valid Loss: 2.8903720378875732\n",
      "Epoch 313 train Loss: 2.890371561050415\n",
      "Epoch 313 valid Loss: 2.8903720378875732\n",
      "Epoch 314 train Loss: 2.890371561050415\n",
      "Epoch 314 valid Loss: 2.8903720378875732\n",
      "Epoch 315 train Loss: 2.890371561050415\n",
      "Epoch 315 valid Loss: 2.8903720378875732\n",
      "Epoch 316 train Loss: 2.890371561050415\n",
      "Epoch 316 valid Loss: 2.8903720378875732\n",
      "Epoch 317 train Loss: 2.890371561050415\n",
      "Epoch 317 valid Loss: 2.8903720378875732\n",
      "Epoch 318 train Loss: 2.890371561050415\n",
      "Epoch 318 valid Loss: 2.8903720378875732\n",
      "Epoch 319 train Loss: 2.890371561050415\n",
      "Epoch 319 valid Loss: 2.890371561050415\n",
      "Epoch 320 train Loss: 2.890371561050415\n",
      "Epoch 320 valid Loss: 2.8903720378875732\n",
      "Epoch 321 train Loss: 2.890371561050415\n",
      "Epoch 321 valid Loss: 2.8903720378875732\n",
      "Epoch 322 train Loss: 2.890371561050415\n",
      "Epoch 322 valid Loss: 2.8903720378875732\n",
      "Epoch 323 train Loss: 2.890371561050415\n",
      "Epoch 323 valid Loss: 2.8903720378875732\n",
      "Epoch 324 train Loss: 2.890371561050415\n",
      "Epoch 324 valid Loss: 2.8903720378875732\n",
      "Epoch 325 train Loss: 2.890371561050415\n",
      "Epoch 325 valid Loss: 2.890371561050415\n",
      "Epoch 326 train Loss: 2.890371561050415\n",
      "Epoch 326 valid Loss: 2.8903720378875732\n",
      "Epoch 327 train Loss: 2.890371799468994\n",
      "Epoch 327 valid Loss: 2.8903720378875732\n",
      "Epoch 328 train Loss: 2.890371561050415\n",
      "Epoch 328 valid Loss: 2.8903720378875732\n",
      "Epoch 329 train Loss: 2.890371561050415\n",
      "Epoch 329 valid Loss: 2.8903720378875732\n",
      "Epoch 330 train Loss: 2.890371561050415\n",
      "Epoch 330 valid Loss: 2.8903720378875732\n",
      "Epoch 331 train Loss: 2.890371561050415\n",
      "Epoch 331 valid Loss: 2.8903720378875732\n",
      "Epoch 332 train Loss: 2.890371561050415\n",
      "Epoch 332 valid Loss: 2.8903720378875732\n",
      "Epoch 333 train Loss: 2.890371561050415\n",
      "Epoch 333 valid Loss: 2.890371561050415\n",
      "Epoch 334 train Loss: 2.890371561050415\n",
      "Epoch 334 valid Loss: 2.8903720378875732\n",
      "Epoch 335 train Loss: 2.890371561050415\n",
      "Epoch 335 valid Loss: 2.8903720378875732\n",
      "Epoch 336 train Loss: 2.890371561050415\n",
      "Epoch 336 valid Loss: 2.8903720378875732\n",
      "Epoch 337 train Loss: 2.890371561050415\n",
      "Epoch 337 valid Loss: 2.8903720378875732\n",
      "Epoch 338 train Loss: 2.890371561050415\n",
      "Epoch 338 valid Loss: 2.8903720378875732\n",
      "Epoch 339 train Loss: 2.890371561050415\n",
      "Epoch 339 valid Loss: 2.8903720378875732\n",
      "Epoch 340 train Loss: 2.890371561050415\n",
      "Epoch 340 valid Loss: 2.8903720378875732\n",
      "Epoch 341 train Loss: 2.890371561050415\n",
      "Epoch 341 valid Loss: 2.8903720378875732\n",
      "Epoch 342 train Loss: 2.890371561050415\n",
      "Epoch 342 valid Loss: 2.8903720378875732\n",
      "Epoch 343 train Loss: 2.890371561050415\n",
      "Epoch 343 valid Loss: 2.8903720378875732\n",
      "Epoch 344 train Loss: 2.890371561050415\n",
      "Epoch 344 valid Loss: 2.8903720378875732\n",
      "Epoch 345 train Loss: 2.890371561050415\n",
      "Epoch 345 valid Loss: 2.8903720378875732\n",
      "Epoch 346 train Loss: 2.890371561050415\n",
      "Epoch 346 valid Loss: 2.8903720378875732\n",
      "Epoch 347 train Loss: 2.890371561050415\n",
      "Epoch 347 valid Loss: 2.8903720378875732\n",
      "Epoch 348 train Loss: 2.890371561050415\n",
      "Epoch 348 valid Loss: 2.8903720378875732\n",
      "Epoch 349 train Loss: 2.890371561050415\n",
      "Epoch 349 valid Loss: 2.8903720378875732\n",
      "Epoch 350 train Loss: 2.890371561050415\n",
      "Epoch 350 valid Loss: 2.8903720378875732\n",
      "Epoch 351 train Loss: 2.890371561050415\n",
      "Epoch 351 valid Loss: 2.8903720378875732\n",
      "Epoch 352 train Loss: 2.890371561050415\n",
      "Epoch 352 valid Loss: 2.8903720378875732\n",
      "Epoch 353 train Loss: 2.890371561050415\n",
      "Epoch 353 valid Loss: 2.8903720378875732\n",
      "Epoch 354 train Loss: 2.890371561050415\n",
      "Epoch 354 valid Loss: 2.8903720378875732\n",
      "Epoch 355 train Loss: 2.890371561050415\n",
      "Epoch 355 valid Loss: 2.8903720378875732\n",
      "Epoch 356 train Loss: 2.890371561050415\n",
      "Epoch 356 valid Loss: 2.890371561050415\n",
      "Epoch 357 train Loss: 2.890371561050415\n",
      "Epoch 357 valid Loss: 2.8903720378875732\n",
      "Epoch 358 train Loss: 2.890371561050415\n",
      "Epoch 358 valid Loss: 2.8903720378875732\n",
      "Epoch 359 train Loss: 2.890371561050415\n",
      "Epoch 359 valid Loss: 2.8903720378875732\n",
      "Epoch 360 train Loss: 2.890371561050415\n",
      "Epoch 360 valid Loss: 2.8903720378875732\n",
      "Epoch 361 train Loss: 2.890371561050415\n",
      "Epoch 361 valid Loss: 2.8903720378875732\n",
      "Epoch 362 train Loss: 2.890371561050415\n",
      "Epoch 362 valid Loss: 2.8903720378875732\n",
      "Epoch 363 train Loss: 2.890371561050415\n",
      "Epoch 363 valid Loss: 2.8903720378875732\n",
      "Epoch 364 train Loss: 2.890371561050415\n",
      "Epoch 364 valid Loss: 2.8903720378875732\n",
      "Epoch 365 train Loss: 2.890371561050415\n",
      "Epoch 365 valid Loss: 2.8903720378875732\n",
      "Epoch 366 train Loss: 2.890371561050415\n",
      "Epoch 366 valid Loss: 2.8903720378875732\n",
      "Epoch 367 train Loss: 2.890371561050415\n",
      "Epoch 367 valid Loss: 2.8903720378875732\n",
      "Epoch 368 train Loss: 2.890371561050415\n",
      "Epoch 368 valid Loss: 2.8903720378875732\n",
      "Epoch 369 train Loss: 2.890371561050415\n",
      "Epoch 369 valid Loss: 2.8903720378875732\n",
      "Epoch 370 train Loss: 2.890371561050415\n",
      "Epoch 370 valid Loss: 2.8903720378875732\n",
      "Epoch 371 train Loss: 2.890371561050415\n",
      "Epoch 371 valid Loss: 2.8903720378875732\n",
      "Epoch 372 train Loss: 2.890371561050415\n",
      "Epoch 372 valid Loss: 2.8903720378875732\n",
      "Epoch 373 train Loss: 2.890371561050415\n",
      "Epoch 373 valid Loss: 2.8903720378875732\n",
      "Epoch 374 train Loss: 2.890371561050415\n",
      "Epoch 374 valid Loss: 2.8903720378875732\n",
      "Epoch 375 train Loss: 2.890371561050415\n",
      "Epoch 375 valid Loss: 2.8903720378875732\n",
      "Epoch 376 train Loss: 2.890371561050415\n",
      "Epoch 376 valid Loss: 2.8903720378875732\n",
      "Epoch 377 train Loss: 2.890371561050415\n",
      "Epoch 377 valid Loss: 2.8903720378875732\n",
      "Epoch 378 train Loss: 2.890371561050415\n",
      "Epoch 378 valid Loss: 2.8903720378875732\n",
      "Epoch 379 train Loss: 2.890371561050415\n",
      "Epoch 379 valid Loss: 2.890371561050415\n",
      "Epoch 380 train Loss: 2.890371561050415\n",
      "Epoch 380 valid Loss: 2.890371561050415\n",
      "Epoch 381 train Loss: 2.890371561050415\n",
      "Epoch 381 valid Loss: 2.8903720378875732\n",
      "Epoch 382 train Loss: 2.890371561050415\n",
      "Epoch 382 valid Loss: 2.8903720378875732\n",
      "Epoch 383 train Loss: 2.890371561050415\n",
      "Epoch 383 valid Loss: 2.8903720378875732\n",
      "Epoch 384 train Loss: 2.890371561050415\n",
      "Epoch 384 valid Loss: 2.8903720378875732\n",
      "Epoch 385 train Loss: 2.890371561050415\n",
      "Epoch 385 valid Loss: 2.8903720378875732\n",
      "Epoch 386 train Loss: 2.890371561050415\n",
      "Epoch 386 valid Loss: 2.8903720378875732\n",
      "Epoch 387 train Loss: 2.890371561050415\n",
      "Epoch 387 valid Loss: 2.8903720378875732\n",
      "Epoch 388 train Loss: 2.890371561050415\n",
      "Epoch 388 valid Loss: 2.8903720378875732\n",
      "Epoch 389 train Loss: 2.890371561050415\n",
      "Epoch 389 valid Loss: 2.8903720378875732\n",
      "Epoch 390 train Loss: 2.890371561050415\n",
      "Epoch 390 valid Loss: 2.8903720378875732\n",
      "Epoch 391 train Loss: 2.890371561050415\n",
      "Epoch 391 valid Loss: 2.8903720378875732\n",
      "Epoch 392 train Loss: 2.890371561050415\n",
      "Epoch 392 valid Loss: 2.8903720378875732\n",
      "Epoch 393 train Loss: 2.890371561050415\n",
      "Epoch 393 valid Loss: 2.8903720378875732\n",
      "Epoch 394 train Loss: 2.890371561050415\n",
      "Epoch 394 valid Loss: 2.8903720378875732\n",
      "Epoch 395 train Loss: 2.890371561050415\n",
      "Epoch 395 valid Loss: 2.8903720378875732\n",
      "Epoch 396 train Loss: 2.890371561050415\n",
      "Epoch 396 valid Loss: 2.8903720378875732\n",
      "Epoch 397 train Loss: 2.890371561050415\n",
      "Epoch 397 valid Loss: 2.8903720378875732\n",
      "Epoch 398 train Loss: 2.890371561050415\n",
      "Epoch 398 valid Loss: 2.8903720378875732\n",
      "Epoch 399 train Loss: 2.890371561050415\n",
      "Epoch 399 valid Loss: 2.8903720378875732\n",
      "Epoch 400 train Loss: 2.890371561050415\n",
      "Epoch 400 valid Loss: 2.8903720378875732\n",
      "Epoch 401 train Loss: 2.890371561050415\n",
      "Epoch 401 valid Loss: 2.8903720378875732\n",
      "Epoch 402 train Loss: 2.890371561050415\n",
      "Epoch 402 valid Loss: 2.8903720378875732\n",
      "Epoch 403 train Loss: 2.890371561050415\n",
      "Epoch 403 valid Loss: 2.8903720378875732\n",
      "Epoch 404 train Loss: 2.890371561050415\n",
      "Epoch 404 valid Loss: 2.8903720378875732\n",
      "Epoch 405 train Loss: 2.890371561050415\n",
      "Epoch 405 valid Loss: 2.8903720378875732\n",
      "Epoch 406 train Loss: 2.890371561050415\n",
      "Epoch 406 valid Loss: 2.8903720378875732\n",
      "Epoch 407 train Loss: 2.890371561050415\n",
      "Epoch 407 valid Loss: 2.8903720378875732\n",
      "Epoch 408 train Loss: 2.890371561050415\n",
      "Epoch 408 valid Loss: 2.8903720378875732\n",
      "Epoch 409 train Loss: 2.890371561050415\n",
      "Epoch 409 valid Loss: 2.8903720378875732\n",
      "Epoch 410 train Loss: 2.890371561050415\n",
      "Epoch 410 valid Loss: 2.8903720378875732\n",
      "Epoch 411 train Loss: 2.890371561050415\n",
      "Epoch 411 valid Loss: 2.8903720378875732\n",
      "Epoch 412 train Loss: 2.890371561050415\n",
      "Epoch 412 valid Loss: 2.8903720378875732\n",
      "Epoch 413 train Loss: 2.890371561050415\n",
      "Epoch 413 valid Loss: 2.8903720378875732\n",
      "Epoch 414 train Loss: 2.890371561050415\n",
      "Epoch 414 valid Loss: 2.8903720378875732\n",
      "Epoch 415 train Loss: 2.890371561050415\n",
      "Epoch 415 valid Loss: 2.8903720378875732\n",
      "Epoch 416 train Loss: 2.890371561050415\n",
      "Epoch 416 valid Loss: 2.8903720378875732\n",
      "Epoch 417 train Loss: 2.890371561050415\n",
      "Epoch 417 valid Loss: 2.8903720378875732\n",
      "Epoch 418 train Loss: 2.890371561050415\n",
      "Epoch 418 valid Loss: 2.890371561050415\n",
      "Epoch 419 train Loss: 2.890371561050415\n",
      "Epoch 419 valid Loss: 2.8903720378875732\n",
      "Epoch 420 train Loss: 2.890371561050415\n",
      "Epoch 420 valid Loss: 2.8903720378875732\n",
      "Epoch 421 train Loss: 2.890371561050415\n",
      "Epoch 421 valid Loss: 2.8903720378875732\n",
      "Epoch 422 train Loss: 2.890371561050415\n",
      "Epoch 422 valid Loss: 2.8903720378875732\n",
      "Epoch 423 train Loss: 2.890371561050415\n",
      "Epoch 423 valid Loss: 2.890371561050415\n",
      "Epoch 424 train Loss: 2.890371561050415\n",
      "Epoch 424 valid Loss: 2.8903720378875732\n",
      "Epoch 425 train Loss: 2.890371561050415\n",
      "Epoch 425 valid Loss: 2.890371561050415\n",
      "Epoch 426 train Loss: 2.890371561050415\n",
      "Epoch 426 valid Loss: 2.8903720378875732\n",
      "Epoch 427 train Loss: 2.890371561050415\n",
      "Epoch 427 valid Loss: 2.8903720378875732\n",
      "Epoch 428 train Loss: 2.890371561050415\n",
      "Epoch 428 valid Loss: 2.8903720378875732\n",
      "Epoch 429 train Loss: 2.890371561050415\n",
      "Epoch 429 valid Loss: 2.8903720378875732\n",
      "Epoch 430 train Loss: 2.890371561050415\n",
      "Epoch 430 valid Loss: 2.8903720378875732\n",
      "Epoch 431 train Loss: 2.890371561050415\n",
      "Epoch 431 valid Loss: 2.8903720378875732\n",
      "Epoch 432 train Loss: 2.890371561050415\n",
      "Epoch 432 valid Loss: 2.8903720378875732\n",
      "Epoch 433 train Loss: 2.890371561050415\n",
      "Epoch 433 valid Loss: 2.8903720378875732\n",
      "Epoch 434 train Loss: 2.890371561050415\n",
      "Epoch 434 valid Loss: 2.8903720378875732\n",
      "Epoch 435 train Loss: 2.890371561050415\n",
      "Epoch 435 valid Loss: 2.8903720378875732\n",
      "Epoch 436 train Loss: 2.890371561050415\n",
      "Epoch 436 valid Loss: 2.8903720378875732\n",
      "Epoch 437 train Loss: 2.890371561050415\n",
      "Epoch 437 valid Loss: 2.8903720378875732\n",
      "Epoch 438 train Loss: 2.890371561050415\n",
      "Epoch 438 valid Loss: 2.8903720378875732\n",
      "Epoch 439 train Loss: 2.890371561050415\n",
      "Epoch 439 valid Loss: 2.8903720378875732\n",
      "Epoch 440 train Loss: 2.890371561050415\n",
      "Epoch 440 valid Loss: 2.8903720378875732\n",
      "Epoch 441 train Loss: 2.890371561050415\n",
      "Epoch 441 valid Loss: 2.8903720378875732\n",
      "Epoch 442 train Loss: 2.890371561050415\n",
      "Epoch 442 valid Loss: 2.8903720378875732\n",
      "Epoch 443 train Loss: 2.890371561050415\n",
      "Epoch 443 valid Loss: 2.8903720378875732\n",
      "Epoch 444 train Loss: 2.890371561050415\n",
      "Epoch 444 valid Loss: 2.8903720378875732\n",
      "Epoch 445 train Loss: 2.890371561050415\n",
      "Epoch 445 valid Loss: 2.8903720378875732\n",
      "Epoch 446 train Loss: 2.890371561050415\n",
      "Epoch 446 valid Loss: 2.8903720378875732\n",
      "Epoch 447 train Loss: 2.890371561050415\n",
      "Epoch 447 valid Loss: 2.8903720378875732\n",
      "Epoch 448 train Loss: 2.890371561050415\n",
      "Epoch 448 valid Loss: 2.8903720378875732\n",
      "Epoch 449 train Loss: 2.890371561050415\n",
      "Epoch 449 valid Loss: 2.8903720378875732\n",
      "Epoch 450 train Loss: 2.890371561050415\n",
      "Epoch 450 valid Loss: 2.8903720378875732\n",
      "Epoch 451 train Loss: 2.890371561050415\n",
      "Epoch 451 valid Loss: 2.8903720378875732\n",
      "Epoch 452 train Loss: 2.890371561050415\n",
      "Epoch 452 valid Loss: 2.8903720378875732\n",
      "Epoch 453 train Loss: 2.890371561050415\n",
      "Epoch 453 valid Loss: 2.8903720378875732\n",
      "Epoch 454 train Loss: 2.890371561050415\n",
      "Epoch 454 valid Loss: 2.8903720378875732\n",
      "Epoch 455 train Loss: 2.890371561050415\n",
      "Epoch 455 valid Loss: 2.8903720378875732\n",
      "Epoch 456 train Loss: 2.890371561050415\n",
      "Epoch 456 valid Loss: 2.8903720378875732\n",
      "Epoch 457 train Loss: 2.890371561050415\n",
      "Epoch 457 valid Loss: 2.8903720378875732\n",
      "Epoch 458 train Loss: 2.890371561050415\n",
      "Epoch 458 valid Loss: 2.890371561050415\n",
      "Epoch 459 train Loss: 2.890371561050415\n",
      "Epoch 459 valid Loss: 2.8903720378875732\n",
      "Epoch 460 train Loss: 2.890371561050415\n",
      "Epoch 460 valid Loss: 2.890371561050415\n",
      "Epoch 461 train Loss: 2.890371561050415\n",
      "Epoch 461 valid Loss: 2.8903720378875732\n",
      "Epoch 462 train Loss: 2.890371561050415\n",
      "Epoch 462 valid Loss: 2.8903720378875732\n",
      "Epoch 463 train Loss: 2.890371561050415\n",
      "Epoch 463 valid Loss: 2.8903720378875732\n",
      "Epoch 464 train Loss: 2.890371799468994\n",
      "Epoch 464 valid Loss: 2.8903720378875732\n",
      "Epoch 465 train Loss: 2.890371561050415\n",
      "Epoch 465 valid Loss: 2.8903720378875732\n",
      "Epoch 466 train Loss: 2.890371561050415\n",
      "Epoch 466 valid Loss: 2.8903720378875732\n",
      "Epoch 467 train Loss: 2.890371561050415\n",
      "Epoch 467 valid Loss: 2.8903720378875732\n",
      "Epoch 468 train Loss: 2.890371561050415\n",
      "Epoch 468 valid Loss: 2.8903720378875732\n",
      "Epoch 469 train Loss: 2.890371561050415\n",
      "Epoch 469 valid Loss: 2.8903720378875732\n",
      "Epoch 470 train Loss: 2.890371561050415\n",
      "Epoch 470 valid Loss: 2.8903720378875732\n",
      "Epoch 471 train Loss: 2.890371561050415\n",
      "Epoch 471 valid Loss: 2.8903720378875732\n",
      "Epoch 472 train Loss: 2.890371561050415\n",
      "Epoch 472 valid Loss: 2.8903720378875732\n",
      "Epoch 473 train Loss: 2.890371561050415\n",
      "Epoch 473 valid Loss: 2.8903720378875732\n",
      "Epoch 474 train Loss: 2.890371561050415\n",
      "Epoch 474 valid Loss: 2.8903720378875732\n",
      "Epoch 475 train Loss: 2.890371561050415\n",
      "Epoch 475 valid Loss: 2.8903720378875732\n",
      "Epoch 476 train Loss: 2.890371561050415\n",
      "Epoch 476 valid Loss: 2.8903720378875732\n",
      "Epoch 477 train Loss: 2.890371561050415\n",
      "Epoch 477 valid Loss: 2.8903720378875732\n",
      "Epoch 478 train Loss: 2.890371561050415\n",
      "Epoch 478 valid Loss: 2.8903720378875732\n",
      "Epoch 479 train Loss: 2.890371561050415\n",
      "Epoch 479 valid Loss: 2.8903720378875732\n",
      "Epoch 480 train Loss: 2.890371561050415\n",
      "Epoch 480 valid Loss: 2.8903720378875732\n",
      "Epoch 481 train Loss: 2.890371561050415\n",
      "Epoch 481 valid Loss: 2.8903720378875732\n",
      "Epoch 482 train Loss: 2.890371561050415\n",
      "Epoch 482 valid Loss: 2.8903720378875732\n",
      "Epoch 483 train Loss: 2.890371561050415\n",
      "Epoch 483 valid Loss: 2.8903720378875732\n",
      "Epoch 484 train Loss: 2.890371561050415\n",
      "Epoch 484 valid Loss: 2.8903720378875732\n",
      "Epoch 485 train Loss: 2.890371561050415\n",
      "Epoch 485 valid Loss: 2.8903720378875732\n",
      "Epoch 486 train Loss: 2.890371561050415\n",
      "Epoch 486 valid Loss: 2.8903720378875732\n",
      "Epoch 487 train Loss: 2.890371561050415\n",
      "Epoch 487 valid Loss: 2.8903720378875732\n",
      "Epoch 488 train Loss: 2.890371561050415\n",
      "Epoch 488 valid Loss: 2.8903720378875732\n",
      "Epoch 489 train Loss: 2.890371561050415\n",
      "Epoch 489 valid Loss: 2.8903720378875732\n",
      "Epoch 490 train Loss: 2.890371561050415\n",
      "Epoch 490 valid Loss: 2.8903720378875732\n",
      "Epoch 491 train Loss: 2.890371561050415\n",
      "Epoch 491 valid Loss: 2.8903720378875732\n",
      "Epoch 492 train Loss: 2.890371561050415\n",
      "Epoch 492 valid Loss: 2.890371561050415\n",
      "Epoch 493 train Loss: 2.890371561050415\n",
      "Epoch 493 valid Loss: 2.8903720378875732\n",
      "Epoch 494 train Loss: 2.890371561050415\n",
      "Epoch 494 valid Loss: 2.8903720378875732\n",
      "Epoch 495 train Loss: 2.890371561050415\n",
      "Epoch 495 valid Loss: 2.8903720378875732\n",
      "Epoch 496 train Loss: 2.890371561050415\n",
      "Epoch 496 valid Loss: 2.8903720378875732\n",
      "Epoch 497 train Loss: 2.890371561050415\n",
      "Epoch 497 valid Loss: 2.8903720378875732\n",
      "Epoch 498 train Loss: 2.890371561050415\n",
      "Epoch 498 valid Loss: 2.8903720378875732\n",
      "Epoch 499 train Loss: 2.890371561050415\n",
      "Epoch 499 valid Loss: 2.890371561050415\n",
      "Epoch 500 train Loss: 2.890371561050415\n",
      "Epoch 500 valid Loss: 2.8903720378875732\n",
      "Epoch 501 train Loss: 2.890371561050415\n",
      "Epoch 501 valid Loss: 2.8903720378875732\n",
      "Epoch 502 train Loss: 2.890371561050415\n",
      "Epoch 502 valid Loss: 2.8903720378875732\n",
      "Epoch 503 train Loss: 2.890371561050415\n",
      "Epoch 503 valid Loss: 2.8903720378875732\n",
      "Epoch 504 train Loss: 2.890371561050415\n",
      "Epoch 504 valid Loss: 2.8903720378875732\n",
      "Epoch 505 train Loss: 2.890371561050415\n",
      "Epoch 505 valid Loss: 2.8903720378875732\n",
      "Epoch 506 train Loss: 2.890371561050415\n",
      "Epoch 506 valid Loss: 2.8903720378875732\n",
      "Epoch 507 train Loss: 2.890371561050415\n",
      "Epoch 507 valid Loss: 2.8903720378875732\n",
      "Epoch 508 train Loss: 2.890371561050415\n",
      "Epoch 508 valid Loss: 2.8903720378875732\n",
      "Epoch 509 train Loss: 2.890371561050415\n",
      "Epoch 509 valid Loss: 2.8903720378875732\n",
      "Epoch 510 train Loss: 2.890371561050415\n",
      "Epoch 510 valid Loss: 2.8903720378875732\n",
      "Epoch 511 train Loss: 2.890371561050415\n",
      "Epoch 511 valid Loss: 2.8903720378875732\n",
      "Epoch 512 train Loss: 2.890371561050415\n",
      "Epoch 512 valid Loss: 2.8903720378875732\n",
      "Epoch 513 train Loss: 2.890371561050415\n",
      "Epoch 513 valid Loss: 2.8903720378875732\n",
      "Epoch 514 train Loss: 2.890371561050415\n",
      "Epoch 514 valid Loss: 2.8903720378875732\n",
      "Epoch 515 train Loss: 2.890371561050415\n",
      "Epoch 515 valid Loss: 2.8903720378875732\n",
      "Epoch 516 train Loss: 2.890371561050415\n",
      "Epoch 516 valid Loss: 2.8903720378875732\n",
      "Epoch 517 train Loss: 2.890371561050415\n",
      "Epoch 517 valid Loss: 2.8903720378875732\n",
      "Epoch 518 train Loss: 2.890371561050415\n",
      "Epoch 518 valid Loss: 2.8903720378875732\n",
      "Epoch 519 train Loss: 2.890371561050415\n",
      "Epoch 519 valid Loss: 2.8903720378875732\n",
      "Epoch 520 train Loss: 2.890371561050415\n",
      "Epoch 520 valid Loss: 2.8903720378875732\n",
      "Epoch 521 train Loss: 2.890371561050415\n",
      "Epoch 521 valid Loss: 2.8903720378875732\n",
      "Epoch 522 train Loss: 2.890371561050415\n",
      "Epoch 522 valid Loss: 2.8903720378875732\n",
      "Epoch 523 train Loss: 2.890371561050415\n",
      "Epoch 523 valid Loss: 2.8903720378875732\n",
      "Epoch 524 train Loss: 2.890371561050415\n",
      "Epoch 524 valid Loss: 2.8903720378875732\n",
      "Epoch 525 train Loss: 2.890371561050415\n",
      "Epoch 525 valid Loss: 2.8903720378875732\n",
      "Epoch 526 train Loss: 2.890371561050415\n",
      "Epoch 526 valid Loss: 2.8903720378875732\n",
      "Epoch 527 train Loss: 2.890371561050415\n",
      "Epoch 527 valid Loss: 2.8903720378875732\n",
      "Epoch 528 train Loss: 2.890371799468994\n",
      "Epoch 528 valid Loss: 2.890371561050415\n",
      "Epoch 529 train Loss: 2.890371561050415\n",
      "Epoch 529 valid Loss: 2.8903720378875732\n",
      "Epoch 530 train Loss: 2.890371561050415\n",
      "Epoch 530 valid Loss: 2.8903720378875732\n",
      "Epoch 531 train Loss: 2.890371561050415\n",
      "Epoch 531 valid Loss: 2.8903720378875732\n",
      "Epoch 532 train Loss: 2.890371561050415\n",
      "Epoch 532 valid Loss: 2.8903720378875732\n",
      "Epoch 533 train Loss: 2.890371561050415\n",
      "Epoch 533 valid Loss: 2.8903720378875732\n",
      "Epoch 534 train Loss: 2.890371561050415\n",
      "Epoch 534 valid Loss: 2.8903720378875732\n",
      "Epoch 535 train Loss: 2.890371561050415\n",
      "Epoch 535 valid Loss: 2.8903720378875732\n",
      "Epoch 536 train Loss: 2.890371561050415\n",
      "Epoch 536 valid Loss: 2.8903720378875732\n",
      "Epoch 537 train Loss: 2.890371561050415\n",
      "Epoch 537 valid Loss: 2.8903720378875732\n",
      "Epoch 538 train Loss: 2.890371561050415\n",
      "Epoch 538 valid Loss: 2.8903720378875732\n",
      "Epoch 539 train Loss: 2.890371561050415\n",
      "Epoch 539 valid Loss: 2.8903720378875732\n",
      "Epoch 540 train Loss: 2.890371561050415\n",
      "Epoch 540 valid Loss: 2.8903720378875732\n",
      "Epoch 541 train Loss: 2.890371561050415\n",
      "Epoch 541 valid Loss: 2.8903720378875732\n",
      "Epoch 542 train Loss: 2.890371561050415\n",
      "Epoch 542 valid Loss: 2.8903720378875732\n",
      "Epoch 543 train Loss: 2.890371561050415\n",
      "Epoch 543 valid Loss: 2.8903720378875732\n",
      "Epoch 544 train Loss: 2.890371561050415\n",
      "Epoch 544 valid Loss: 2.8903720378875732\n",
      "Epoch 545 train Loss: 2.890371561050415\n",
      "Epoch 545 valid Loss: 2.890371561050415\n",
      "Epoch 546 train Loss: 2.890371561050415\n",
      "Epoch 546 valid Loss: 2.8903720378875732\n",
      "Epoch 547 train Loss: 2.890371561050415\n",
      "Epoch 547 valid Loss: 2.8903720378875732\n",
      "Epoch 548 train Loss: 2.890371561050415\n",
      "Epoch 548 valid Loss: 2.8903720378875732\n",
      "Epoch 549 train Loss: 2.890371561050415\n",
      "Epoch 549 valid Loss: 2.8903720378875732\n",
      "Epoch 550 train Loss: 2.890371561050415\n",
      "Epoch 550 valid Loss: 2.8903720378875732\n",
      "Epoch 551 train Loss: 2.890371561050415\n",
      "Epoch 551 valid Loss: 2.890371561050415\n",
      "Epoch 552 train Loss: 2.890371561050415\n",
      "Epoch 552 valid Loss: 2.8903720378875732\n",
      "Epoch 553 train Loss: 2.890371561050415\n",
      "Epoch 553 valid Loss: 2.8903720378875732\n",
      "Epoch 554 train Loss: 2.890371561050415\n",
      "Epoch 554 valid Loss: 2.8903720378875732\n",
      "Epoch 555 train Loss: 2.890371561050415\n",
      "Epoch 555 valid Loss: 2.8903720378875732\n",
      "Epoch 556 train Loss: 2.890371561050415\n",
      "Epoch 556 valid Loss: 2.8903720378875732\n",
      "Epoch 557 train Loss: 2.890371561050415\n",
      "Epoch 557 valid Loss: 2.8903720378875732\n",
      "Epoch 558 train Loss: 2.890371561050415\n",
      "Epoch 558 valid Loss: 2.8903720378875732\n",
      "Epoch 559 train Loss: 2.890371561050415\n",
      "Epoch 559 valid Loss: 2.8903720378875732\n",
      "Epoch 560 train Loss: 2.890371561050415\n",
      "Epoch 560 valid Loss: 2.8903720378875732\n",
      "Epoch 561 train Loss: 2.890371561050415\n",
      "Epoch 561 valid Loss: 2.8903720378875732\n",
      "Epoch 562 train Loss: 2.890371561050415\n",
      "Epoch 562 valid Loss: 2.8903720378875732\n",
      "Epoch 563 train Loss: 2.890371561050415\n",
      "Epoch 563 valid Loss: 2.8903720378875732\n",
      "Epoch 564 train Loss: 2.890371561050415\n",
      "Epoch 564 valid Loss: 2.8903720378875732\n",
      "Epoch 565 train Loss: 2.890371561050415\n",
      "Epoch 565 valid Loss: 2.8903720378875732\n",
      "Epoch 566 train Loss: 2.890371561050415\n",
      "Epoch 566 valid Loss: 2.8903720378875732\n",
      "Epoch 567 train Loss: 2.890371561050415\n",
      "Epoch 567 valid Loss: 2.8903720378875732\n",
      "Epoch 568 train Loss: 2.890371561050415\n",
      "Epoch 568 valid Loss: 2.890371561050415\n",
      "Epoch 569 train Loss: 2.890371561050415\n",
      "Epoch 569 valid Loss: 2.890371561050415\n",
      "Epoch 570 train Loss: 2.890371561050415\n",
      "Epoch 570 valid Loss: 2.8903720378875732\n",
      "Epoch 571 train Loss: 2.890371561050415\n",
      "Epoch 571 valid Loss: 2.890371561050415\n",
      "Epoch 572 train Loss: 2.890371561050415\n",
      "Epoch 572 valid Loss: 2.8903720378875732\n",
      "Epoch 573 train Loss: 2.890371561050415\n",
      "Epoch 573 valid Loss: 2.8903720378875732\n",
      "Epoch 574 train Loss: 2.890371561050415\n",
      "Epoch 574 valid Loss: 2.8903720378875732\n",
      "Epoch 575 train Loss: 2.890371561050415\n",
      "Epoch 575 valid Loss: 2.8903720378875732\n",
      "Epoch 576 train Loss: 2.890371561050415\n",
      "Epoch 576 valid Loss: 2.8903720378875732\n",
      "Epoch 577 train Loss: 2.890371561050415\n",
      "Epoch 577 valid Loss: 2.890371561050415\n",
      "Epoch 578 train Loss: 2.890371561050415\n",
      "Epoch 578 valid Loss: 2.8903720378875732\n",
      "Epoch 579 train Loss: 2.890371561050415\n",
      "Epoch 579 valid Loss: 2.8903720378875732\n",
      "Epoch 580 train Loss: 2.890371561050415\n",
      "Epoch 580 valid Loss: 2.8903720378875732\n",
      "Epoch 581 train Loss: 2.890371561050415\n",
      "Epoch 581 valid Loss: 2.8903720378875732\n",
      "Epoch 582 train Loss: 2.890371561050415\n",
      "Epoch 582 valid Loss: 2.8903720378875732\n",
      "Epoch 583 train Loss: 2.890371561050415\n",
      "Epoch 583 valid Loss: 2.8903720378875732\n",
      "Epoch 584 train Loss: 2.890371561050415\n",
      "Epoch 584 valid Loss: 2.8903720378875732\n",
      "Epoch 585 train Loss: 2.890371561050415\n",
      "Epoch 585 valid Loss: 2.8903720378875732\n",
      "Epoch 586 train Loss: 2.890371561050415\n",
      "Epoch 586 valid Loss: 2.8903720378875732\n",
      "Epoch 587 train Loss: 2.890371561050415\n",
      "Epoch 587 valid Loss: 2.8903720378875732\n",
      "Epoch 588 train Loss: 2.890371561050415\n",
      "Epoch 588 valid Loss: 2.8903720378875732\n",
      "Epoch 589 train Loss: 2.890371561050415\n",
      "Epoch 589 valid Loss: 2.8903720378875732\n",
      "Epoch 590 train Loss: 2.890371561050415\n",
      "Epoch 590 valid Loss: 2.8903720378875732\n",
      "Epoch 591 train Loss: 2.890371561050415\n",
      "Epoch 591 valid Loss: 2.8903720378875732\n",
      "Epoch 592 train Loss: 2.890371561050415\n",
      "Epoch 592 valid Loss: 2.890371561050415\n",
      "Epoch 593 train Loss: 2.890371799468994\n",
      "Epoch 593 valid Loss: 2.8903720378875732\n",
      "Epoch 594 train Loss: 2.890371561050415\n",
      "Epoch 594 valid Loss: 2.8903720378875732\n",
      "Epoch 595 train Loss: 2.890371561050415\n",
      "Epoch 595 valid Loss: 2.8903720378875732\n",
      "Epoch 596 train Loss: 2.890371561050415\n",
      "Epoch 596 valid Loss: 2.8903720378875732\n",
      "Epoch 597 train Loss: 2.890371561050415\n",
      "Epoch 597 valid Loss: 2.8903720378875732\n",
      "Epoch 598 train Loss: 2.890371561050415\n",
      "Epoch 598 valid Loss: 2.8903720378875732\n",
      "Epoch 599 train Loss: 2.890371561050415\n",
      "Epoch 599 valid Loss: 2.8903720378875732\n",
      "Epoch 600 train Loss: 2.890371561050415\n",
      "Epoch 600 valid Loss: 2.890371561050415\n",
      "Epoch 601 train Loss: 2.890371561050415\n",
      "Epoch 601 valid Loss: 2.8903720378875732\n",
      "Epoch 602 train Loss: 2.890371561050415\n",
      "Epoch 602 valid Loss: 2.8903720378875732\n",
      "Epoch 603 train Loss: 2.890371561050415\n",
      "Epoch 603 valid Loss: 2.8903720378875732\n",
      "Epoch 604 train Loss: 2.890371561050415\n",
      "Epoch 604 valid Loss: 2.8903720378875732\n",
      "Epoch 605 train Loss: 2.890371561050415\n",
      "Epoch 605 valid Loss: 2.8903720378875732\n",
      "Epoch 606 train Loss: 2.890371561050415\n",
      "Epoch 606 valid Loss: 2.8903720378875732\n",
      "Epoch 607 train Loss: 2.890371561050415\n",
      "Epoch 607 valid Loss: 2.8903720378875732\n",
      "Epoch 608 train Loss: 2.890371561050415\n",
      "Epoch 608 valid Loss: 2.8903720378875732\n",
      "Epoch 609 train Loss: 2.890371561050415\n",
      "Epoch 609 valid Loss: 2.8903720378875732\n",
      "Epoch 610 train Loss: 2.890371561050415\n",
      "Epoch 610 valid Loss: 2.8903720378875732\n",
      "Epoch 611 train Loss: 2.890371561050415\n",
      "Epoch 611 valid Loss: 2.8903720378875732\n",
      "Epoch 612 train Loss: 2.890371561050415\n",
      "Epoch 612 valid Loss: 2.8903720378875732\n",
      "Epoch 613 train Loss: 2.890371561050415\n",
      "Epoch 613 valid Loss: 2.8903720378875732\n",
      "Epoch 614 train Loss: 2.890371561050415\n",
      "Epoch 614 valid Loss: 2.8903720378875732\n",
      "Epoch 615 train Loss: 2.890371561050415\n",
      "Epoch 615 valid Loss: 2.8903720378875732\n",
      "Epoch 616 train Loss: 2.890371561050415\n",
      "Epoch 616 valid Loss: 2.8903720378875732\n",
      "Epoch 617 train Loss: 2.890371561050415\n",
      "Epoch 617 valid Loss: 2.8903720378875732\n",
      "Epoch 618 train Loss: 2.890371561050415\n",
      "Epoch 618 valid Loss: 2.8903720378875732\n",
      "Epoch 619 train Loss: 2.890371561050415\n",
      "Epoch 619 valid Loss: 2.8903720378875732\n",
      "Epoch 620 train Loss: 2.890371561050415\n",
      "Epoch 620 valid Loss: 2.890371561050415\n",
      "Epoch 621 train Loss: 2.890371561050415\n",
      "Epoch 621 valid Loss: 2.890371561050415\n",
      "Epoch 622 train Loss: 2.890371561050415\n",
      "Epoch 622 valid Loss: 2.8903720378875732\n",
      "Epoch 623 train Loss: 2.890371561050415\n",
      "Epoch 623 valid Loss: 2.8903720378875732\n",
      "Epoch 624 train Loss: 2.890371561050415\n",
      "Epoch 624 valid Loss: 2.8903720378875732\n",
      "Epoch 625 train Loss: 2.890371561050415\n",
      "Epoch 625 valid Loss: 2.8903720378875732\n",
      "Epoch 626 train Loss: 2.890371561050415\n",
      "Epoch 626 valid Loss: 2.8903720378875732\n",
      "Epoch 627 train Loss: 2.890371561050415\n",
      "Epoch 627 valid Loss: 2.8903720378875732\n",
      "Epoch 628 train Loss: 2.890371561050415\n",
      "Epoch 628 valid Loss: 2.8903720378875732\n",
      "Epoch 629 train Loss: 2.890371561050415\n",
      "Epoch 629 valid Loss: 2.8903720378875732\n",
      "Epoch 630 train Loss: 2.890371561050415\n",
      "Epoch 630 valid Loss: 2.8903720378875732\n",
      "Epoch 631 train Loss: 2.890371561050415\n",
      "Epoch 631 valid Loss: 2.8903720378875732\n",
      "Epoch 632 train Loss: 2.890371561050415\n",
      "Epoch 632 valid Loss: 2.8903720378875732\n",
      "Epoch 633 train Loss: 2.890371561050415\n",
      "Epoch 633 valid Loss: 2.8903720378875732\n",
      "Epoch 634 train Loss: 2.890371561050415\n",
      "Epoch 634 valid Loss: 2.8903720378875732\n",
      "Epoch 635 train Loss: 2.890371561050415\n",
      "Epoch 635 valid Loss: 2.8903720378875732\n",
      "Epoch 636 train Loss: 2.890371561050415\n",
      "Epoch 636 valid Loss: 2.8903720378875732\n",
      "Epoch 637 train Loss: 2.890371561050415\n",
      "Epoch 637 valid Loss: 2.8903720378875732\n",
      "Epoch 638 train Loss: 2.890371561050415\n",
      "Epoch 638 valid Loss: 2.8903720378875732\n",
      "Epoch 639 train Loss: 2.890371561050415\n",
      "Epoch 639 valid Loss: 2.8903720378875732\n",
      "Epoch 640 train Loss: 2.890371561050415\n",
      "Epoch 640 valid Loss: 2.8903720378875732\n",
      "Epoch 641 train Loss: 2.890371561050415\n",
      "Epoch 641 valid Loss: 2.8903720378875732\n",
      "Epoch 642 train Loss: 2.890371561050415\n",
      "Epoch 642 valid Loss: 2.8903720378875732\n",
      "Epoch 643 train Loss: 2.890371561050415\n",
      "Epoch 643 valid Loss: 2.8903720378875732\n",
      "Epoch 644 train Loss: 2.890371561050415\n",
      "Epoch 644 valid Loss: 2.8903720378875732\n",
      "Epoch 645 train Loss: 2.890371561050415\n",
      "Epoch 645 valid Loss: 2.8903720378875732\n",
      "Epoch 646 train Loss: 2.890371561050415\n",
      "Epoch 646 valid Loss: 2.8903720378875732\n",
      "Epoch 647 train Loss: 2.890371561050415\n",
      "Epoch 647 valid Loss: 2.8903720378875732\n",
      "Epoch 648 train Loss: 2.890371561050415\n",
      "Epoch 648 valid Loss: 2.8903720378875732\n",
      "Epoch 649 train Loss: 2.890371561050415\n",
      "Epoch 649 valid Loss: 2.8903720378875732\n",
      "Epoch 650 train Loss: 2.890371561050415\n",
      "Epoch 650 valid Loss: 2.8903720378875732\n",
      "Epoch 651 train Loss: 2.890371561050415\n",
      "Epoch 651 valid Loss: 2.8903720378875732\n",
      "Epoch 652 train Loss: 2.890371561050415\n",
      "Epoch 652 valid Loss: 2.8903720378875732\n",
      "Epoch 653 train Loss: 2.890371561050415\n",
      "Epoch 653 valid Loss: 2.8903720378875732\n",
      "Epoch 654 train Loss: 2.890371561050415\n",
      "Epoch 654 valid Loss: 2.890371561050415\n",
      "Epoch 655 train Loss: 2.890371561050415\n",
      "Epoch 655 valid Loss: 2.8903720378875732\n",
      "Epoch 656 train Loss: 2.890371561050415\n",
      "Epoch 656 valid Loss: 2.8903720378875732\n",
      "Epoch 657 train Loss: 2.890371561050415\n",
      "Epoch 657 valid Loss: 2.8903720378875732\n",
      "Epoch 658 train Loss: 2.890371561050415\n",
      "Epoch 658 valid Loss: 2.8903720378875732\n",
      "Epoch 659 train Loss: 2.890371561050415\n",
      "Epoch 659 valid Loss: 2.8903720378875732\n",
      "Epoch 660 train Loss: 2.890371561050415\n",
      "Epoch 660 valid Loss: 2.890371561050415\n",
      "Epoch 661 train Loss: 2.890371561050415\n",
      "Epoch 661 valid Loss: 2.8903720378875732\n",
      "Epoch 662 train Loss: 2.890371561050415\n",
      "Epoch 662 valid Loss: 2.8903720378875732\n",
      "Epoch 663 train Loss: 2.890371561050415\n",
      "Epoch 663 valid Loss: 2.8903720378875732\n",
      "Epoch 664 train Loss: 2.890371561050415\n",
      "Epoch 664 valid Loss: 2.8903720378875732\n",
      "Epoch 665 train Loss: 2.890371561050415\n",
      "Epoch 665 valid Loss: 2.8903720378875732\n",
      "Epoch 666 train Loss: 2.890371561050415\n",
      "Epoch 666 valid Loss: 2.8903720378875732\n",
      "Epoch 667 train Loss: 2.890371561050415\n",
      "Epoch 667 valid Loss: 2.890371561050415\n",
      "Epoch 668 train Loss: 2.890371561050415\n",
      "Epoch 668 valid Loss: 2.8903720378875732\n",
      "Epoch 669 train Loss: 2.890371561050415\n",
      "Epoch 669 valid Loss: 2.890371561050415\n",
      "Epoch 670 train Loss: 2.890371561050415\n",
      "Epoch 670 valid Loss: 2.890371561050415\n",
      "Epoch 671 train Loss: 2.890371561050415\n",
      "Epoch 671 valid Loss: 2.8903720378875732\n",
      "Epoch 672 train Loss: 2.890371561050415\n",
      "Epoch 672 valid Loss: 2.8903720378875732\n",
      "Epoch 673 train Loss: 2.890371561050415\n",
      "Epoch 673 valid Loss: 2.8903720378875732\n",
      "Epoch 674 train Loss: 2.890371561050415\n",
      "Epoch 674 valid Loss: 2.8903720378875732\n",
      "Epoch 675 train Loss: 2.890371561050415\n",
      "Epoch 675 valid Loss: 2.8903720378875732\n",
      "Epoch 676 train Loss: 2.890371561050415\n",
      "Epoch 676 valid Loss: 2.8903720378875732\n",
      "Epoch 677 train Loss: 2.890371561050415\n",
      "Epoch 677 valid Loss: 2.8903720378875732\n",
      "Epoch 678 train Loss: 2.890371561050415\n",
      "Epoch 678 valid Loss: 2.8903720378875732\n",
      "Epoch 679 train Loss: 2.890371561050415\n",
      "Epoch 679 valid Loss: 2.8903720378875732\n",
      "Epoch 680 train Loss: 2.890371561050415\n",
      "Epoch 680 valid Loss: 2.8903720378875732\n",
      "Epoch 681 train Loss: 2.890371561050415\n",
      "Epoch 681 valid Loss: 2.8903720378875732\n",
      "Epoch 682 train Loss: 2.890371561050415\n",
      "Epoch 682 valid Loss: 2.8903720378875732\n",
      "Epoch 683 train Loss: 2.890371561050415\n",
      "Epoch 683 valid Loss: 2.8903720378875732\n",
      "Epoch 684 train Loss: 2.890371561050415\n",
      "Epoch 684 valid Loss: 2.8903720378875732\n",
      "Epoch 685 train Loss: 2.890371561050415\n",
      "Epoch 685 valid Loss: 2.8903720378875732\n",
      "Epoch 686 train Loss: 2.890371561050415\n",
      "Epoch 686 valid Loss: 2.8903720378875732\n",
      "Epoch 687 train Loss: 2.890371561050415\n",
      "Epoch 687 valid Loss: 2.8903720378875732\n",
      "Epoch 688 train Loss: 2.890371561050415\n",
      "Epoch 688 valid Loss: 2.890371561050415\n",
      "Epoch 689 train Loss: 2.890371561050415\n",
      "Epoch 689 valid Loss: 2.8903720378875732\n",
      "Epoch 690 train Loss: 2.890371561050415\n",
      "Epoch 690 valid Loss: 2.8903720378875732\n",
      "Epoch 691 train Loss: 2.890371561050415\n",
      "Epoch 691 valid Loss: 2.8903720378875732\n",
      "Epoch 692 train Loss: 2.890371561050415\n",
      "Epoch 692 valid Loss: 2.890371561050415\n",
      "Epoch 693 train Loss: 2.890371561050415\n",
      "Epoch 693 valid Loss: 2.8903720378875732\n",
      "Epoch 694 train Loss: 2.890371561050415\n",
      "Epoch 694 valid Loss: 2.8903720378875732\n",
      "Epoch 695 train Loss: 2.890371561050415\n",
      "Epoch 695 valid Loss: 2.8903720378875732\n",
      "Epoch 696 train Loss: 2.890371561050415\n",
      "Epoch 696 valid Loss: 2.8903720378875732\n",
      "Epoch 697 train Loss: 2.890371561050415\n",
      "Epoch 697 valid Loss: 2.8903720378875732\n",
      "Epoch 698 train Loss: 2.890371561050415\n",
      "Epoch 698 valid Loss: 2.8903720378875732\n",
      "Epoch 699 train Loss: 2.890371561050415\n",
      "Epoch 699 valid Loss: 2.8903720378875732\n",
      "Epoch 700 train Loss: 2.890371561050415\n",
      "Epoch 700 valid Loss: 2.8903720378875732\n",
      "Epoch 701 train Loss: 2.890371561050415\n",
      "Epoch 701 valid Loss: 2.8903720378875732\n",
      "Epoch 702 train Loss: 2.890371561050415\n",
      "Epoch 702 valid Loss: 2.890371561050415\n",
      "Epoch 703 train Loss: 2.890371561050415\n",
      "Epoch 703 valid Loss: 2.8903720378875732\n",
      "Epoch 704 train Loss: 2.890371561050415\n",
      "Epoch 704 valid Loss: 2.8903720378875732\n",
      "Epoch 705 train Loss: 2.890371561050415\n",
      "Epoch 705 valid Loss: 2.8903720378875732\n",
      "Epoch 706 train Loss: 2.890371561050415\n",
      "Epoch 706 valid Loss: 2.8903720378875732\n",
      "Epoch 707 train Loss: 2.890371561050415\n",
      "Epoch 707 valid Loss: 2.8903720378875732\n",
      "Epoch 708 train Loss: 2.890371561050415\n",
      "Epoch 708 valid Loss: 2.8903720378875732\n",
      "Epoch 709 train Loss: 2.890371561050415\n",
      "Epoch 709 valid Loss: 2.890371561050415\n",
      "Epoch 710 train Loss: 2.890371561050415\n",
      "Epoch 710 valid Loss: 2.890371561050415\n",
      "Epoch 711 train Loss: 2.890371561050415\n",
      "Epoch 711 valid Loss: 2.8903720378875732\n",
      "Epoch 712 train Loss: 2.890371561050415\n",
      "Epoch 712 valid Loss: 2.8903720378875732\n",
      "Epoch 713 train Loss: 2.890371561050415\n",
      "Epoch 713 valid Loss: 2.8903720378875732\n",
      "Epoch 714 train Loss: 2.890371561050415\n",
      "Epoch 714 valid Loss: 2.8903720378875732\n",
      "Epoch 715 train Loss: 2.890371561050415\n",
      "Epoch 715 valid Loss: 2.8903720378875732\n",
      "Epoch 716 train Loss: 2.890371561050415\n",
      "Epoch 716 valid Loss: 2.8903720378875732\n",
      "Epoch 717 train Loss: 2.890371561050415\n",
      "Epoch 717 valid Loss: 2.890371561050415\n",
      "Epoch 718 train Loss: 2.890371561050415\n",
      "Epoch 718 valid Loss: 2.8903720378875732\n",
      "Epoch 719 train Loss: 2.890371561050415\n",
      "Epoch 719 valid Loss: 2.8903720378875732\n",
      "Epoch 720 train Loss: 2.890371561050415\n",
      "Epoch 720 valid Loss: 2.8903720378875732\n",
      "Epoch 721 train Loss: 2.890371561050415\n",
      "Epoch 721 valid Loss: 2.890371561050415\n",
      "Epoch 722 train Loss: 2.890371561050415\n",
      "Epoch 722 valid Loss: 2.8903720378875732\n",
      "Epoch 723 train Loss: 2.890371561050415\n",
      "Epoch 723 valid Loss: 2.8903720378875732\n",
      "Epoch 724 train Loss: 2.890371561050415\n",
      "Epoch 724 valid Loss: 2.8903720378875732\n",
      "Epoch 725 train Loss: 2.890371561050415\n",
      "Epoch 725 valid Loss: 2.8903720378875732\n",
      "Epoch 726 train Loss: 2.890371561050415\n",
      "Epoch 726 valid Loss: 2.8903720378875732\n",
      "Epoch 727 train Loss: 2.890371561050415\n",
      "Epoch 727 valid Loss: 2.890371561050415\n",
      "Epoch 728 train Loss: 2.890371561050415\n",
      "Epoch 728 valid Loss: 2.8903720378875732\n",
      "Epoch 729 train Loss: 2.890371561050415\n",
      "Epoch 729 valid Loss: 2.8903720378875732\n",
      "Epoch 730 train Loss: 2.890371561050415\n",
      "Epoch 730 valid Loss: 2.890371561050415\n",
      "Epoch 731 train Loss: 2.890371561050415\n",
      "Epoch 731 valid Loss: 2.8903720378875732\n",
      "Epoch 732 train Loss: 2.890371561050415\n",
      "Epoch 732 valid Loss: 2.8903720378875732\n",
      "Epoch 733 train Loss: 2.890371561050415\n",
      "Epoch 733 valid Loss: 2.8903720378875732\n",
      "Epoch 734 train Loss: 2.890371561050415\n",
      "Epoch 734 valid Loss: 2.8903720378875732\n",
      "Epoch 735 train Loss: 2.890371561050415\n",
      "Epoch 735 valid Loss: 2.8903720378875732\n",
      "Epoch 736 train Loss: 2.890371561050415\n",
      "Epoch 736 valid Loss: 2.8903720378875732\n",
      "Epoch 737 train Loss: 2.890371561050415\n",
      "Epoch 737 valid Loss: 2.8903720378875732\n",
      "Epoch 738 train Loss: 2.890371561050415\n",
      "Epoch 738 valid Loss: 2.8903720378875732\n",
      "Epoch 739 train Loss: 2.890371561050415\n",
      "Epoch 739 valid Loss: 2.8903720378875732\n",
      "Epoch 740 train Loss: 2.890371561050415\n",
      "Epoch 740 valid Loss: 2.8903720378875732\n",
      "Epoch 741 train Loss: 2.890371561050415\n",
      "Epoch 741 valid Loss: 2.8903720378875732\n",
      "Epoch 742 train Loss: 2.890371561050415\n",
      "Epoch 742 valid Loss: 2.8903720378875732\n",
      "Epoch 743 train Loss: 2.890371561050415\n",
      "Epoch 743 valid Loss: 2.8903720378875732\n",
      "Epoch 744 train Loss: 2.890371561050415\n",
      "Epoch 744 valid Loss: 2.8903720378875732\n",
      "Epoch 745 train Loss: 2.890371561050415\n",
      "Epoch 745 valid Loss: 2.8903720378875732\n",
      "Epoch 746 train Loss: 2.890371561050415\n",
      "Epoch 746 valid Loss: 2.8903720378875732\n",
      "Epoch 747 train Loss: 2.890371561050415\n",
      "Epoch 747 valid Loss: 2.890371561050415\n",
      "Epoch 748 train Loss: 2.890371561050415\n",
      "Epoch 748 valid Loss: 2.8903720378875732\n",
      "Epoch 749 train Loss: 2.890371561050415\n",
      "Epoch 749 valid Loss: 2.8903720378875732\n",
      "Epoch 750 train Loss: 2.890371561050415\n",
      "Epoch 750 valid Loss: 2.8903720378875732\n",
      "Epoch 751 train Loss: 2.890371561050415\n",
      "Epoch 751 valid Loss: 2.8903722763061523\n",
      "Epoch 752 train Loss: 2.890371561050415\n",
      "Epoch 752 valid Loss: 2.8903720378875732\n",
      "Epoch 753 train Loss: 2.890371561050415\n",
      "Epoch 753 valid Loss: 2.8903720378875732\n",
      "Epoch 754 train Loss: 2.890371561050415\n",
      "Epoch 754 valid Loss: 2.8903720378875732\n",
      "Epoch 755 train Loss: 2.890371561050415\n",
      "Epoch 755 valid Loss: 2.8903720378875732\n",
      "Epoch 756 train Loss: 2.890371561050415\n",
      "Epoch 756 valid Loss: 2.8903720378875732\n",
      "Epoch 757 train Loss: 2.890371561050415\n",
      "Epoch 757 valid Loss: 2.8903720378875732\n",
      "Epoch 758 train Loss: 2.890371561050415\n",
      "Epoch 758 valid Loss: 2.8903720378875732\n",
      "Epoch 759 train Loss: 2.890371561050415\n",
      "Epoch 759 valid Loss: 2.8903720378875732\n",
      "Epoch 760 train Loss: 2.890371561050415\n",
      "Epoch 760 valid Loss: 2.8903720378875732\n",
      "Epoch 761 train Loss: 2.890371561050415\n",
      "Epoch 761 valid Loss: 2.8903720378875732\n",
      "Epoch 762 train Loss: 2.890371561050415\n",
      "Epoch 762 valid Loss: 2.8903720378875732\n",
      "Epoch 763 train Loss: 2.890371561050415\n",
      "Epoch 763 valid Loss: 2.8903720378875732\n",
      "Epoch 764 train Loss: 2.890371561050415\n",
      "Epoch 764 valid Loss: 2.8903720378875732\n",
      "Epoch 765 train Loss: 2.890371561050415\n",
      "Epoch 765 valid Loss: 2.8903720378875732\n",
      "Epoch 766 train Loss: 2.890371561050415\n",
      "Epoch 766 valid Loss: 2.890371561050415\n",
      "Epoch 767 train Loss: 2.890371561050415\n",
      "Epoch 767 valid Loss: 2.8903720378875732\n",
      "Epoch 768 train Loss: 2.890371561050415\n",
      "Epoch 768 valid Loss: 2.8903720378875732\n",
      "Epoch 769 train Loss: 2.890371561050415\n",
      "Epoch 769 valid Loss: 2.8903720378875732\n",
      "Epoch 770 train Loss: 2.890371561050415\n",
      "Epoch 770 valid Loss: 2.8903720378875732\n",
      "Epoch 771 train Loss: 2.890371561050415\n",
      "Epoch 771 valid Loss: 2.8903720378875732\n",
      "Epoch 772 train Loss: 2.890371561050415\n",
      "Epoch 772 valid Loss: 2.8903720378875732\n",
      "Epoch 773 train Loss: 2.890371561050415\n",
      "Epoch 773 valid Loss: 2.8903720378875732\n",
      "Epoch 774 train Loss: 2.890371561050415\n",
      "Epoch 774 valid Loss: 2.8903720378875732\n",
      "Epoch 775 train Loss: 2.890371561050415\n",
      "Epoch 775 valid Loss: 2.8903720378875732\n",
      "Epoch 776 train Loss: 2.890371561050415\n",
      "Epoch 776 valid Loss: 2.8903720378875732\n",
      "Epoch 777 train Loss: 2.890371561050415\n",
      "Epoch 777 valid Loss: 2.890371561050415\n",
      "Epoch 778 train Loss: 2.890371561050415\n",
      "Epoch 778 valid Loss: 2.8903720378875732\n",
      "Epoch 779 train Loss: 2.890371561050415\n",
      "Epoch 779 valid Loss: 2.8903720378875732\n",
      "Epoch 780 train Loss: 2.890371561050415\n",
      "Epoch 780 valid Loss: 2.8903720378875732\n",
      "Epoch 781 train Loss: 2.890371561050415\n",
      "Epoch 781 valid Loss: 2.8903720378875732\n",
      "Epoch 782 train Loss: 2.890371561050415\n",
      "Epoch 782 valid Loss: 2.8903720378875732\n",
      "Epoch 783 train Loss: 2.890371561050415\n",
      "Epoch 783 valid Loss: 2.8903720378875732\n",
      "Epoch 784 train Loss: 2.890371561050415\n",
      "Epoch 784 valid Loss: 2.890371561050415\n",
      "Epoch 785 train Loss: 2.890371561050415\n",
      "Epoch 785 valid Loss: 2.8903720378875732\n",
      "Epoch 786 train Loss: 2.890371561050415\n",
      "Epoch 786 valid Loss: 2.8903720378875732\n",
      "Epoch 787 train Loss: 2.890371561050415\n",
      "Epoch 787 valid Loss: 2.8903720378875732\n",
      "Epoch 788 train Loss: 2.890371561050415\n",
      "Epoch 788 valid Loss: 2.8903720378875732\n",
      "Epoch 789 train Loss: 2.890371561050415\n",
      "Epoch 789 valid Loss: 2.8903720378875732\n",
      "Epoch 790 train Loss: 2.890371561050415\n",
      "Epoch 790 valid Loss: 2.8903720378875732\n",
      "Epoch 791 train Loss: 2.890371561050415\n",
      "Epoch 791 valid Loss: 2.8903720378875732\n",
      "Epoch 792 train Loss: 2.890371561050415\n",
      "Epoch 792 valid Loss: 2.8903720378875732\n",
      "Epoch 793 train Loss: 2.890371561050415\n",
      "Epoch 793 valid Loss: 2.8903720378875732\n",
      "Epoch 794 train Loss: 2.890371561050415\n",
      "Epoch 794 valid Loss: 2.8903720378875732\n",
      "Epoch 795 train Loss: 2.890371561050415\n",
      "Epoch 795 valid Loss: 2.8903720378875732\n",
      "Epoch 796 train Loss: 2.890371561050415\n",
      "Epoch 796 valid Loss: 2.8903720378875732\n",
      "Epoch 797 train Loss: 2.890371561050415\n",
      "Epoch 797 valid Loss: 2.8903720378875732\n",
      "Epoch 798 train Loss: 2.890371561050415\n",
      "Epoch 798 valid Loss: 2.8903720378875732\n",
      "Epoch 799 train Loss: 2.890371561050415\n",
      "Epoch 799 valid Loss: 2.8903720378875732\n",
      "Epoch 800 train Loss: 2.890371561050415\n",
      "Epoch 800 valid Loss: 2.8903720378875732\n",
      "Epoch 801 train Loss: 2.890371561050415\n",
      "Epoch 801 valid Loss: 2.890371561050415\n",
      "Epoch 802 train Loss: 2.890371561050415\n",
      "Epoch 802 valid Loss: 2.8903720378875732\n",
      "Epoch 803 train Loss: 2.890371561050415\n",
      "Epoch 803 valid Loss: 2.8903720378875732\n",
      "Epoch 804 train Loss: 2.890371561050415\n",
      "Epoch 804 valid Loss: 2.8903720378875732\n",
      "Epoch 805 train Loss: 2.890371561050415\n",
      "Epoch 805 valid Loss: 2.8903720378875732\n",
      "Epoch 806 train Loss: 2.890371561050415\n",
      "Epoch 806 valid Loss: 2.890371561050415\n",
      "Epoch 807 train Loss: 2.890371561050415\n",
      "Epoch 807 valid Loss: 2.8903720378875732\n",
      "Epoch 808 train Loss: 2.890371561050415\n",
      "Epoch 808 valid Loss: 2.890371561050415\n",
      "Epoch 809 train Loss: 2.890371561050415\n",
      "Epoch 809 valid Loss: 2.8903720378875732\n",
      "Epoch 810 train Loss: 2.890371561050415\n",
      "Epoch 810 valid Loss: 2.8903720378875732\n",
      "Epoch 811 train Loss: 2.890371561050415\n",
      "Epoch 811 valid Loss: 2.8903720378875732\n",
      "Epoch 812 train Loss: 2.890371561050415\n",
      "Epoch 812 valid Loss: 2.8903720378875732\n",
      "Epoch 813 train Loss: 2.890371561050415\n",
      "Epoch 813 valid Loss: 2.8903720378875732\n",
      "Epoch 814 train Loss: 2.890371561050415\n",
      "Epoch 814 valid Loss: 2.890371561050415\n",
      "Epoch 815 train Loss: 2.890371561050415\n",
      "Epoch 815 valid Loss: 2.8903720378875732\n",
      "Epoch 816 train Loss: 2.890371561050415\n",
      "Epoch 816 valid Loss: 2.8903720378875732\n",
      "Epoch 817 train Loss: 2.890371561050415\n",
      "Epoch 817 valid Loss: 2.8903720378875732\n",
      "Epoch 818 train Loss: 2.890371561050415\n",
      "Epoch 818 valid Loss: 2.8903720378875732\n",
      "Epoch 819 train Loss: 2.890371561050415\n",
      "Epoch 819 valid Loss: 2.8903720378875732\n",
      "Epoch 820 train Loss: 2.890371561050415\n",
      "Epoch 820 valid Loss: 2.8903720378875732\n",
      "Epoch 821 train Loss: 2.890371561050415\n",
      "Epoch 821 valid Loss: 2.8903720378875732\n",
      "Epoch 822 train Loss: 2.890371561050415\n",
      "Epoch 822 valid Loss: 2.8903720378875732\n",
      "Epoch 823 train Loss: 2.890371561050415\n",
      "Epoch 823 valid Loss: 2.8903720378875732\n",
      "Epoch 824 train Loss: 2.890371561050415\n",
      "Epoch 824 valid Loss: 2.8903720378875732\n",
      "Epoch 825 train Loss: 2.890371561050415\n",
      "Epoch 825 valid Loss: 2.8903720378875732\n",
      "Epoch 826 train Loss: 2.890371561050415\n",
      "Epoch 826 valid Loss: 2.8903720378875732\n",
      "Epoch 827 train Loss: 2.890371561050415\n",
      "Epoch 827 valid Loss: 2.8903720378875732\n",
      "Epoch 828 train Loss: 2.890371561050415\n",
      "Epoch 828 valid Loss: 2.8903720378875732\n",
      "Epoch 829 train Loss: 2.890371561050415\n",
      "Epoch 829 valid Loss: 2.8903720378875732\n",
      "Epoch 830 train Loss: 2.890371561050415\n",
      "Epoch 830 valid Loss: 2.8903720378875732\n",
      "Epoch 831 train Loss: 2.890371561050415\n",
      "Epoch 831 valid Loss: 2.8903720378875732\n",
      "Epoch 832 train Loss: 2.890371561050415\n",
      "Epoch 832 valid Loss: 2.8903720378875732\n",
      "Epoch 833 train Loss: 2.890371561050415\n",
      "Epoch 833 valid Loss: 2.8903720378875732\n",
      "Epoch 834 train Loss: 2.890371561050415\n",
      "Epoch 834 valid Loss: 2.8903720378875732\n",
      "Epoch 835 train Loss: 2.890371561050415\n",
      "Epoch 835 valid Loss: 2.8903720378875732\n",
      "Epoch 836 train Loss: 2.890371561050415\n",
      "Epoch 836 valid Loss: 2.890371561050415\n",
      "Epoch 837 train Loss: 2.890371561050415\n",
      "Epoch 837 valid Loss: 2.8903720378875732\n",
      "Epoch 838 train Loss: 2.890371561050415\n",
      "Epoch 838 valid Loss: 2.8903720378875732\n",
      "Epoch 839 train Loss: 2.890371561050415\n",
      "Epoch 839 valid Loss: 2.8903720378875732\n",
      "Epoch 840 train Loss: 2.890371561050415\n",
      "Epoch 840 valid Loss: 2.8903720378875732\n",
      "Epoch 841 train Loss: 2.890371561050415\n",
      "Epoch 841 valid Loss: 2.8903720378875732\n",
      "Epoch 842 train Loss: 2.890371561050415\n",
      "Epoch 842 valid Loss: 2.8903720378875732\n",
      "Epoch 843 train Loss: 2.890371561050415\n",
      "Epoch 843 valid Loss: 2.8903720378875732\n",
      "Epoch 844 train Loss: 2.890371561050415\n",
      "Epoch 844 valid Loss: 2.8903720378875732\n",
      "Epoch 845 train Loss: 2.890371561050415\n",
      "Epoch 845 valid Loss: 2.8903720378875732\n",
      "Epoch 846 train Loss: 2.890371561050415\n",
      "Epoch 846 valid Loss: 2.8903720378875732\n",
      "Epoch 847 train Loss: 2.890371561050415\n",
      "Epoch 847 valid Loss: 2.8903720378875732\n",
      "Epoch 848 train Loss: 2.890371561050415\n",
      "Epoch 848 valid Loss: 2.8903720378875732\n",
      "Epoch 849 train Loss: 2.890371561050415\n",
      "Epoch 849 valid Loss: 2.8903720378875732\n",
      "Epoch 850 train Loss: 2.890371561050415\n",
      "Epoch 850 valid Loss: 2.8903720378875732\n",
      "Epoch 851 train Loss: 2.890371561050415\n",
      "Epoch 851 valid Loss: 2.8903720378875732\n",
      "Epoch 852 train Loss: 2.890371561050415\n",
      "Epoch 852 valid Loss: 2.8903720378875732\n",
      "Epoch 853 train Loss: 2.890371561050415\n",
      "Epoch 853 valid Loss: 2.8903720378875732\n",
      "Epoch 854 train Loss: 2.890371561050415\n",
      "Epoch 854 valid Loss: 2.8903720378875732\n",
      "Epoch 855 train Loss: 2.890371561050415\n",
      "Epoch 855 valid Loss: 2.8903720378875732\n",
      "Epoch 856 train Loss: 2.890371561050415\n",
      "Epoch 856 valid Loss: 2.8903720378875732\n",
      "Epoch 857 train Loss: 2.890371561050415\n",
      "Epoch 857 valid Loss: 2.8903720378875732\n",
      "Epoch 858 train Loss: 2.890371561050415\n",
      "Epoch 858 valid Loss: 2.8903720378875732\n",
      "Epoch 859 train Loss: 2.890371561050415\n",
      "Epoch 859 valid Loss: 2.8903720378875732\n",
      "Epoch 860 train Loss: 2.890371561050415\n",
      "Epoch 860 valid Loss: 2.8903720378875732\n",
      "Epoch 861 train Loss: 2.890371561050415\n",
      "Epoch 861 valid Loss: 2.8903720378875732\n",
      "Epoch 862 train Loss: 2.890371561050415\n",
      "Epoch 862 valid Loss: 2.8903720378875732\n",
      "Epoch 863 train Loss: 2.890371561050415\n",
      "Epoch 863 valid Loss: 2.8903720378875732\n",
      "Epoch 864 train Loss: 2.890371561050415\n",
      "Epoch 864 valid Loss: 2.8903720378875732\n",
      "Epoch 865 train Loss: 2.890371561050415\n",
      "Epoch 865 valid Loss: 2.8903720378875732\n",
      "Epoch 866 train Loss: 2.890371561050415\n",
      "Epoch 866 valid Loss: 2.8903720378875732\n",
      "Epoch 867 train Loss: 2.890371561050415\n",
      "Epoch 867 valid Loss: 2.8903720378875732\n",
      "Epoch 868 train Loss: 2.890371561050415\n",
      "Epoch 868 valid Loss: 2.8903720378875732\n",
      "Epoch 869 train Loss: 2.890371561050415\n",
      "Epoch 869 valid Loss: 2.8903720378875732\n",
      "Epoch 870 train Loss: 2.890371561050415\n",
      "Epoch 870 valid Loss: 2.8903720378875732\n",
      "Epoch 871 train Loss: 2.890371561050415\n",
      "Epoch 871 valid Loss: 2.8903720378875732\n",
      "Epoch 872 train Loss: 2.890371561050415\n",
      "Epoch 872 valid Loss: 2.8903720378875732\n",
      "Epoch 873 train Loss: 2.890371561050415\n",
      "Epoch 873 valid Loss: 2.8903720378875732\n",
      "Epoch 874 train Loss: 2.890371561050415\n",
      "Epoch 874 valid Loss: 2.8903720378875732\n",
      "Epoch 875 train Loss: 2.890371561050415\n",
      "Epoch 875 valid Loss: 2.890371561050415\n",
      "Epoch 876 train Loss: 2.890371561050415\n",
      "Epoch 876 valid Loss: 2.8903720378875732\n",
      "Epoch 877 train Loss: 2.890371561050415\n",
      "Epoch 877 valid Loss: 2.890371561050415\n",
      "Epoch 878 train Loss: 2.890371561050415\n",
      "Epoch 878 valid Loss: 2.8903720378875732\n",
      "Epoch 879 train Loss: 2.890371561050415\n",
      "Epoch 879 valid Loss: 2.8903720378875732\n",
      "Epoch 880 train Loss: 2.890371561050415\n",
      "Epoch 880 valid Loss: 2.8903720378875732\n",
      "Epoch 881 train Loss: 2.890371561050415\n",
      "Epoch 881 valid Loss: 2.8903720378875732\n",
      "Epoch 882 train Loss: 2.890371561050415\n",
      "Epoch 882 valid Loss: 2.8903720378875732\n",
      "Epoch 883 train Loss: 2.890371561050415\n",
      "Epoch 883 valid Loss: 2.890371561050415\n",
      "Epoch 884 train Loss: 2.890371561050415\n",
      "Epoch 884 valid Loss: 2.8903720378875732\n",
      "Epoch 885 train Loss: 2.890371561050415\n",
      "Epoch 885 valid Loss: 2.8903720378875732\n",
      "Epoch 886 train Loss: 2.890371561050415\n",
      "Epoch 886 valid Loss: 2.890371561050415\n",
      "Epoch 887 train Loss: 2.890371561050415\n",
      "Epoch 887 valid Loss: 2.8903720378875732\n",
      "Epoch 888 train Loss: 2.890371561050415\n",
      "Epoch 888 valid Loss: 2.8903720378875732\n",
      "Epoch 889 train Loss: 2.890371561050415\n",
      "Epoch 889 valid Loss: 2.8903720378875732\n",
      "Epoch 890 train Loss: 2.890371561050415\n",
      "Epoch 890 valid Loss: 2.8903720378875732\n",
      "Epoch 891 train Loss: 2.890371561050415\n",
      "Epoch 891 valid Loss: 2.8903720378875732\n",
      "Epoch 892 train Loss: 2.890371561050415\n",
      "Epoch 892 valid Loss: 2.8903720378875732\n",
      "Epoch 893 train Loss: 2.890371561050415\n",
      "Epoch 893 valid Loss: 2.8903720378875732\n",
      "Epoch 894 train Loss: 2.890371561050415\n",
      "Epoch 894 valid Loss: 2.8903720378875732\n",
      "Epoch 895 train Loss: 2.890371561050415\n",
      "Epoch 895 valid Loss: 2.8903720378875732\n",
      "Epoch 896 train Loss: 2.890371561050415\n",
      "Epoch 896 valid Loss: 2.8903720378875732\n",
      "Epoch 897 train Loss: 2.890371561050415\n",
      "Epoch 897 valid Loss: 2.8903720378875732\n",
      "Epoch 898 train Loss: 2.890371561050415\n",
      "Epoch 898 valid Loss: 2.8903720378875732\n",
      "Epoch 899 train Loss: 2.890371561050415\n",
      "Epoch 899 valid Loss: 2.8903720378875732\n",
      "Epoch 900 train Loss: 2.890371561050415\n",
      "Epoch 900 valid Loss: 2.8903720378875732\n",
      "Epoch 901 train Loss: 2.890371561050415\n",
      "Epoch 901 valid Loss: 2.8903720378875732\n",
      "Epoch 902 train Loss: 2.890371561050415\n",
      "Epoch 902 valid Loss: 2.8903720378875732\n",
      "Epoch 903 train Loss: 2.890371561050415\n",
      "Epoch 903 valid Loss: 2.8903720378875732\n",
      "Epoch 904 train Loss: 2.890371561050415\n",
      "Epoch 904 valid Loss: 2.8903720378875732\n",
      "Epoch 905 train Loss: 2.890371561050415\n",
      "Epoch 905 valid Loss: 2.890371561050415\n",
      "Epoch 906 train Loss: 2.890371561050415\n",
      "Epoch 906 valid Loss: 2.8903720378875732\n",
      "Epoch 907 train Loss: 2.890371561050415\n",
      "Epoch 907 valid Loss: 2.8903720378875732\n",
      "Epoch 908 train Loss: 2.890371561050415\n",
      "Epoch 908 valid Loss: 2.8903720378875732\n",
      "Epoch 909 train Loss: 2.890371561050415\n",
      "Epoch 909 valid Loss: 2.8903720378875732\n",
      "Epoch 910 train Loss: 2.890371561050415\n",
      "Epoch 910 valid Loss: 2.8903720378875732\n",
      "Epoch 911 train Loss: 2.890371561050415\n",
      "Epoch 911 valid Loss: 2.8903720378875732\n",
      "Epoch 912 train Loss: 2.890371561050415\n",
      "Epoch 912 valid Loss: 2.8903720378875732\n",
      "Epoch 913 train Loss: 2.890371561050415\n",
      "Epoch 913 valid Loss: 2.8903720378875732\n",
      "Epoch 914 train Loss: 2.890371561050415\n",
      "Epoch 914 valid Loss: 2.8903720378875732\n",
      "Epoch 915 train Loss: 2.890371561050415\n",
      "Epoch 915 valid Loss: 2.8903720378875732\n",
      "Epoch 916 train Loss: 2.890371561050415\n",
      "Epoch 916 valid Loss: 2.8903720378875732\n",
      "Epoch 917 train Loss: 2.890371561050415\n",
      "Epoch 917 valid Loss: 2.8903720378875732\n",
      "Epoch 918 train Loss: 2.890371561050415\n",
      "Epoch 918 valid Loss: 2.8903720378875732\n",
      "Epoch 919 train Loss: 2.890371561050415\n",
      "Epoch 919 valid Loss: 2.8903720378875732\n",
      "Epoch 920 train Loss: 2.890371561050415\n",
      "Epoch 920 valid Loss: 2.8903720378875732\n",
      "Epoch 921 train Loss: 2.890371561050415\n",
      "Epoch 921 valid Loss: 2.8903720378875732\n",
      "Epoch 922 train Loss: 2.890371561050415\n",
      "Epoch 922 valid Loss: 2.8903720378875732\n",
      "Epoch 923 train Loss: 2.890371561050415\n",
      "Epoch 923 valid Loss: 2.8903720378875732\n",
      "Epoch 924 train Loss: 2.890371561050415\n",
      "Epoch 924 valid Loss: 2.8903720378875732\n",
      "Epoch 925 train Loss: 2.890371561050415\n",
      "Epoch 925 valid Loss: 2.8903720378875732\n",
      "Epoch 926 train Loss: 2.890371561050415\n",
      "Epoch 926 valid Loss: 2.8903720378875732\n",
      "Epoch 927 train Loss: 2.890371561050415\n",
      "Epoch 927 valid Loss: 2.8903720378875732\n",
      "Epoch 928 train Loss: 2.890371561050415\n",
      "Epoch 928 valid Loss: 2.8903720378875732\n",
      "Epoch 929 train Loss: 2.890371561050415\n",
      "Epoch 929 valid Loss: 2.8903720378875732\n",
      "Epoch 930 train Loss: 2.890371561050415\n",
      "Epoch 930 valid Loss: 2.890371561050415\n",
      "Epoch 931 train Loss: 2.890371561050415\n",
      "Epoch 931 valid Loss: 2.8903720378875732\n",
      "Epoch 932 train Loss: 2.890371561050415\n",
      "Epoch 932 valid Loss: 2.8903720378875732\n",
      "Epoch 933 train Loss: 2.890371561050415\n",
      "Epoch 933 valid Loss: 2.8903720378875732\n",
      "Epoch 934 train Loss: 2.890371561050415\n",
      "Epoch 934 valid Loss: 2.8903720378875732\n",
      "Epoch 935 train Loss: 2.890371561050415\n",
      "Epoch 935 valid Loss: 2.890371561050415\n",
      "Epoch 936 train Loss: 2.890371561050415\n",
      "Epoch 936 valid Loss: 2.8903720378875732\n",
      "Epoch 937 train Loss: 2.890371561050415\n",
      "Epoch 937 valid Loss: 2.8903720378875732\n",
      "Epoch 938 train Loss: 2.890371561050415\n",
      "Epoch 938 valid Loss: 2.8903720378875732\n",
      "Epoch 939 train Loss: 2.890371561050415\n",
      "Epoch 939 valid Loss: 2.8903720378875732\n",
      "Epoch 940 train Loss: 2.890371561050415\n",
      "Epoch 940 valid Loss: 2.8903720378875732\n",
      "Epoch 941 train Loss: 2.890371561050415\n",
      "Epoch 941 valid Loss: 2.8903720378875732\n",
      "Epoch 942 train Loss: 2.890371561050415\n",
      "Epoch 942 valid Loss: 2.8903720378875732\n",
      "Epoch 943 train Loss: 2.890371561050415\n",
      "Epoch 943 valid Loss: 2.890371561050415\n",
      "Epoch 944 train Loss: 2.890371561050415\n",
      "Epoch 944 valid Loss: 2.8903720378875732\n",
      "Epoch 945 train Loss: 2.890371561050415\n",
      "Epoch 945 valid Loss: 2.8903720378875732\n",
      "Epoch 946 train Loss: 2.890371561050415\n",
      "Epoch 946 valid Loss: 2.8903720378875732\n",
      "Epoch 947 train Loss: 2.890371561050415\n",
      "Epoch 947 valid Loss: 2.8903720378875732\n",
      "Epoch 948 train Loss: 2.890371561050415\n",
      "Epoch 948 valid Loss: 2.8903720378875732\n",
      "Epoch 949 train Loss: 2.890371799468994\n",
      "Epoch 949 valid Loss: 2.890371561050415\n",
      "Epoch 950 train Loss: 2.890371561050415\n",
      "Epoch 950 valid Loss: 2.890371561050415\n",
      "Epoch 951 train Loss: 2.890371561050415\n",
      "Epoch 951 valid Loss: 2.8903720378875732\n",
      "Epoch 952 train Loss: 2.890371561050415\n",
      "Epoch 952 valid Loss: 2.8903720378875732\n",
      "Epoch 953 train Loss: 2.890371561050415\n",
      "Epoch 953 valid Loss: 2.8903720378875732\n",
      "Epoch 954 train Loss: 2.890371561050415\n",
      "Epoch 954 valid Loss: 2.8903720378875732\n",
      "Epoch 955 train Loss: 2.890371561050415\n",
      "Epoch 955 valid Loss: 2.8903720378875732\n",
      "Epoch 956 train Loss: 2.890371561050415\n",
      "Epoch 956 valid Loss: 2.8903720378875732\n",
      "Epoch 957 train Loss: 2.890371561050415\n",
      "Epoch 957 valid Loss: 2.8903720378875732\n",
      "Epoch 958 train Loss: 2.890371561050415\n",
      "Epoch 958 valid Loss: 2.8903720378875732\n",
      "Epoch 959 train Loss: 2.890371561050415\n",
      "Epoch 959 valid Loss: 2.8903720378875732\n",
      "Epoch 960 train Loss: 2.890371561050415\n",
      "Epoch 960 valid Loss: 2.8903720378875732\n",
      "Epoch 961 train Loss: 2.890371561050415\n",
      "Epoch 961 valid Loss: 2.890371561050415\n",
      "Epoch 962 train Loss: 2.890371561050415\n",
      "Epoch 962 valid Loss: 2.8903720378875732\n",
      "Epoch 963 train Loss: 2.890371561050415\n",
      "Epoch 963 valid Loss: 2.8903720378875732\n",
      "Epoch 964 train Loss: 2.890371561050415\n",
      "Epoch 964 valid Loss: 2.8903720378875732\n",
      "Epoch 965 train Loss: 2.890371561050415\n",
      "Epoch 965 valid Loss: 2.8903720378875732\n",
      "Epoch 966 train Loss: 2.890371561050415\n",
      "Epoch 966 valid Loss: 2.8903720378875732\n",
      "Epoch 967 train Loss: 2.890371561050415\n",
      "Epoch 967 valid Loss: 2.8903720378875732\n",
      "Epoch 968 train Loss: 2.890371561050415\n",
      "Epoch 968 valid Loss: 2.8903720378875732\n",
      "Epoch 969 train Loss: 2.890371561050415\n",
      "Epoch 969 valid Loss: 2.8903720378875732\n",
      "Epoch 970 train Loss: 2.890371561050415\n",
      "Epoch 970 valid Loss: 2.8903720378875732\n",
      "Epoch 971 train Loss: 2.890371561050415\n",
      "Epoch 971 valid Loss: 2.8903720378875732\n",
      "Epoch 972 train Loss: 2.890371561050415\n",
      "Epoch 972 valid Loss: 2.8903720378875732\n",
      "Epoch 973 train Loss: 2.890371561050415\n",
      "Epoch 973 valid Loss: 2.8903720378875732\n",
      "Epoch 974 train Loss: 2.890371561050415\n",
      "Epoch 974 valid Loss: 2.890371561050415\n",
      "Epoch 975 train Loss: 2.890371561050415\n",
      "Epoch 975 valid Loss: 2.8903720378875732\n",
      "Epoch 976 train Loss: 2.890371561050415\n",
      "Epoch 976 valid Loss: 2.8903720378875732\n",
      "Epoch 977 train Loss: 2.890371561050415\n",
      "Epoch 977 valid Loss: 2.8903720378875732\n",
      "Epoch 978 train Loss: 2.890371561050415\n",
      "Epoch 978 valid Loss: 2.890371561050415\n",
      "Epoch 979 train Loss: 2.890371561050415\n",
      "Epoch 979 valid Loss: 2.8903720378875732\n",
      "Epoch 980 train Loss: 2.890371799468994\n",
      "Epoch 980 valid Loss: 2.8903720378875732\n",
      "Epoch 981 train Loss: 2.890371561050415\n",
      "Epoch 981 valid Loss: 2.8903720378875732\n",
      "Epoch 982 train Loss: 2.890371561050415\n",
      "Epoch 982 valid Loss: 2.8903720378875732\n",
      "Epoch 983 train Loss: 2.890371561050415\n",
      "Epoch 983 valid Loss: 2.8903720378875732\n",
      "Epoch 984 train Loss: 2.890371561050415\n",
      "Epoch 984 valid Loss: 2.8903720378875732\n",
      "Epoch 985 train Loss: 2.890371561050415\n",
      "Epoch 985 valid Loss: 2.8903720378875732\n",
      "Epoch 986 train Loss: 2.890371561050415\n",
      "Epoch 986 valid Loss: 2.8903720378875732\n",
      "Epoch 987 train Loss: 2.890371561050415\n",
      "Epoch 987 valid Loss: 2.8903720378875732\n",
      "Epoch 988 train Loss: 2.890371561050415\n",
      "Epoch 988 valid Loss: 2.8903720378875732\n",
      "Epoch 989 train Loss: 2.890371561050415\n",
      "Epoch 989 valid Loss: 2.8903720378875732\n",
      "Epoch 990 train Loss: 2.890371561050415\n",
      "Epoch 990 valid Loss: 2.890371561050415\n",
      "Epoch 991 train Loss: 2.890371561050415\n",
      "Epoch 991 valid Loss: 2.8903720378875732\n",
      "Epoch 992 train Loss: 2.890371561050415\n",
      "Epoch 992 valid Loss: 2.8903720378875732\n",
      "Epoch 993 train Loss: 2.890371561050415\n",
      "Epoch 993 valid Loss: 2.8903720378875732\n",
      "Epoch 994 train Loss: 2.890371561050415\n",
      "Epoch 994 valid Loss: 2.8903720378875732\n",
      "Epoch 995 train Loss: 2.890371561050415\n",
      "Epoch 995 valid Loss: 2.8903720378875732\n",
      "Epoch 996 train Loss: 2.890371561050415\n",
      "Epoch 996 valid Loss: 2.890371561050415\n",
      "Epoch 997 train Loss: 2.890371561050415\n",
      "Epoch 997 valid Loss: 2.890371561050415\n",
      "Epoch 998 train Loss: 2.890371561050415\n",
      "Epoch 998 valid Loss: 2.8903720378875732\n",
      "Epoch 999 train Loss: 2.890371561050415\n",
      "Epoch 999 valid Loss: 2.890371561050415\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "lr = 1e-1\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = SGD(GestureModel.parameters(),lr=lr)\n",
    "def fit():\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_valid_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            xb,yb = batch\n",
    "            preds = GestureModel(xb)\n",
    "            loss = loss_func(preds,yb)\n",
    "            with torch.no_grad():\n",
    "                epoch_train_loss += loss\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        print(f\"Epoch {epoch} train Loss: {epoch_train_loss/len(train_dataloader)}\")\n",
    "\n",
    "        for batch in valid_dataloader:\n",
    "            with torch.no_grad():\n",
    "                xb,yb = batch\n",
    "                preds = GestureModel(xb)\n",
    "                loss = loss_func(preds,yb)\n",
    "                with torch.no_grad():\n",
    "                    epoch_valid_loss += loss\n",
    "        \n",
    "        print(f\"Epoch {epoch} valid Loss: {epoch_valid_loss/len(valid_dataloader)}\")\n",
    "\n",
    "\n",
    "\n",
    "fit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    " \n",
    "ce = nn.CrossEntropyLoss()\n",
    " \n",
    "logits = torch.tensor([[-1.90, -0.29, -2.30], [-0.29, -1.90, -2.30]])\n",
    "target = torch.tensor([[0., 1., 0.], [1., 0., 0.]])\n",
    "logits.shape\n",
    "target.shape\n",
    "# print(\"Cross entropy: %.3f\" % ce(logits, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HandGestures-TVPQwZFT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
